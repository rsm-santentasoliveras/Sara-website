[
  {
    "objectID": "HW2/hw2_questions.html",
    "href": "HW2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nWe compare the number of patents held by firms that are Blueprinty customers vs. non-customers.\n\nimport pandas as pd\n\nblueprinty = pd.read_csv('blueprinty.csv')\nblueprinty.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nThe following histogram shows that Blueprinty customers (orange bars) tend to have slightly more patents than non-customers. The distribution is right-skewed for both groups, but the customer group has more firms with 5+ patents. Additionally, the mean number of patents for non-customers is 3.47 and for customers is 4.13.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Compute mean number of patents by customer status\nmean_patents = blueprinty.groupby(\"iscustomer\")[\"patents\"].mean()\n\n# Plot histogram of number of patents by customer status\nplt.figure(figsize=(10, 5))\nsns.histplot(data=blueprinty, x=\"patents\", hue=\"iscustomer\", multiple=\"dodge\", bins=20)\nplt.title(\"Histogram of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Is Customer\", labels=[\"Non-Customer\", \"Customer\"])\nplt.tight_layout()\n\nmean_patents\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\n\n\n\n\n\n\nObservation: There is a moderate difference in average patent count between customers and non-customers. This suggests that Blueprinty customers may be more successful in generating patents — though this raw difference may reflect underlying differences (e.g. age, region), not necessarily a causal effect of using the software.\nFor this reason, since Blueprinty customers are not selected at random, it may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Boxplot for firm age by customer status\nplt.figure(figsize=(8, 5))\nsns.boxplot(x=\"iscustomer\", y=\"age\", data=blueprinty)\nplt.title(\"Firm Age by Customer Status\")\nplt.xlabel(\"Is Customer\")\nplt.ylabel(\"Firm Age\")\nplt.xticks([0, 1], [\"Non-Customer\", \"Customer\"])\nplt.tight_layout()\n\n# Crosstab for region by customer status\nregion_customer_crosstab = pd.crosstab(blueprinty[\"region\"], blueprinty[\"iscustomer\"], normalize='index') * 100\nregion_customer_crosstab.round(1)\n\n\n\n\n\n\n\niscustomer\n0\n1\n\n\nregion\n\n\n\n\n\n\nMidwest\n83.5\n16.5\n\n\nNortheast\n45.4\n54.6\n\n\nNorthwest\n84.5\n15.5\n\n\nSouth\n81.7\n18.3\n\n\nSouthwest\n82.5\n17.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfter comparing regions and ages by customer status, we can see that, for the firm age, the boxplot shows that Blueprinty customers and non-customers have similar age distributions. However, customers have a slightly higher median age, and both groups show wide variation, with some firms as old as ~50 years. The overall distributions are fairly overlapping.\nOn the other hand, there are large differences in the regional distribution. Northeast has the highest customer share (54.6% of firms in this region are customers). In contrast, in regions like Midwest, Northwest, South, and Southwest, only ~15–18% of firms are customers.\nObservations: There is non-random customer selection: - Age is fairly balanced but might still matter. - Region is clearly associated with customer status, and firms in the Northeast are far more likely to be Blueprinty customers.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWe assume that each observation \\(Y_i\\) follows a Poisson distribution:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda)\n\\]\nThis means the probability mass function is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nAssuming the \\(Y_i\\) are i.i.d., the likelihood function is:\n\\[\n\\mathcal{L}(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{y_i}}{y_i!}\n\\]\nTaking the natural logarithm of the likelihood, we obtain the log-likelihood function:\n\\[\n\\ell(\\lambda) = \\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^n \\left[ y_i \\log(\\lambda) - \\lambda - \\log(y_i!) \\right]\n\\]\nNext, we define the log-likelihood for the Poisson distribution as a function of a single parameter, \\(\\lambda\\), and the observed data, \\(Y\\). This function is maximized to estimate the most likely value of \\(\\lambda\\) that could have generated the data:\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return np.inf  # avoid invalid log(λ) or exp(−λ)\n    Y = np.asarray(Y)\n    log_lik = np.sum(Y * np.log(lmbda) - lmbda - gammaln(Y + 1))\n    return log_lik  # negative for use with minimization\n\nWe then evaluate this function over a range of candidate values for \\(\\lambda\\), using the observed number of patents in the dataset. This helps us visually identify the value of \\(\\lambda\\) that maximizes the log-likelihood:\n\nY = blueprinty[\"patents\"].values\n\nlambda_vals = np.linspace(0.1, 10, 200)\nloglik_vals = [poisson_loglikelihood(lmb, Y) for lmb in lambda_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, color='darkblue')\nplt.title(\"Poisson Log-Likelihood vs. Lambda\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis plot reveals a clear peak in the log-likelihood function, indicating the most likely value of \\(\\lambda\\) given the observed number of patents. The shape confirms that the log-likelihood is concave, making optimization straightforward.\nTo confirm our visual result mathematically, we take the derivative of the log-likelihood function and solve for the value of \\(\\lambda\\) that maximizes it.\nStarting from the log-likelihood:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left[ y_i \\log(\\lambda) - \\lambda - \\log(y_i!) \\right]\n\\]\nTake the derivative with respect to \\(\\lambda\\):\n\\[\n\\frac{d\\ell}{d\\lambda} = \\frac{1}{\\lambda} \\sum_{i=1}^n y_i - n\n\\]\nSet the derivative to zero and solve:\n\\[\n\\begin{gather*}\n\\frac{1}{\\lambda} \\sum_{i=1}^n y_i - n = 0 \\\\\n\\sum_{i=1}^n y_i = n \\lambda \\\\\n\\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n y_i = \\bar{y}\n\\end{gather*}\n\\]\nConclusion: The MLE for \\(\\lambda\\) is simply the sample mean of the observed counts (\\(\\hat{\\lambda}_{\\text{MLE}} = \\bar{y}\\)). This aligns with our numerical result and reinforces the interpretation of \\(\\lambda\\) as the expected number of events (in this case, patents per firm).\nFinally, using scipy.optimize.minimize(), we numerically maximized the Poisson log-likelihood and obtained:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = 3.685\n\\]\nThis matches the sample mean of the observed patent counts, which aligns with our analytical derivation. The optimizer successfully found the value of \\(\\lambda\\) that maximizes the likelihood of observing the data under a Poisson model.\n\nfrom scipy.optimize import minimize\n\n# Define the negative log-likelihood for use with optimizer\ndef neg_poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return np.inf\n    return -poisson_loglikelihood(lmbda, Y)\n\n# Use the same Y from earlier\nY = blueprinty[\"patents\"].values\n\n# Use scipy to minimize the negative log-likelihood\nresult = minimize(fun=neg_poisson_loglikelihood, x0=[1.0], args=(Y,), bounds=[(1e-6, None)])\nlambda_mle = result.x[0]\nlambda_mle\n\n3.6846667021660804\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe now extend our Poisson likelihood to a regression setting, where the rate parameter \\(\\lambda_i\\) varies across observations. Specifically, we assume:\n\\[\n\\lambda_i = \\exp(X_i' \\beta)\n\\]\nThis ensures \\(\\lambda_i &gt; 0\\), as required by the Poisson distribution. The log-likelihood function becomes:\n\\[\n\\ell(\\beta) = \\sum_{i=1}^n \\left[ y_i (X_i' \\beta) - \\exp(X_i' \\beta) - \\log(y_i!) \\right]\n\\]\nWe implement this in Python by defining a function that takes the coefficient vector \\(\\beta\\), the observed count data \\(Y\\), and the covariate matrix \\(X\\):\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_loglik(beta, Y, X):\n    eta = X @ beta\n    eta = np.clip(eta, -20, 20)        # Prevent overflow in exp()\n    lam = np.exp(eta)\n    log_lik = np.sum(Y * eta - lam - gammaln(Y + 1))\n    return -log_lik                   # Return negative for minimizer\n\nTo estimate the Poisson regression model, we use Maximum Likelihood Estimation (MLE) via Python’s scipy.optimize.minimize() function. The goal is to obtain the MLE vector \\(\\hat{\\beta}\\) and compute the standard errors using the inverse of the Hessian matrix. We construct the covariate matrix \\(X\\) as follows:\n\nThe first column is a constant (1’s), enabling an intercept in the model.\nNext are: age, age², dummy variables for all but one region (reference category), and a binary indicator for whether the firm is a Blueprinty customer.\n\n\nimport patsy\n# Create design matrix with intercept, age, age², region dummies, and customer\ny, X = patsy.dmatrices('patents ~ age + I(age**2) + C(region) + iscustomer', data=blueprinty, return_type='dataframe')\nY = y.values.flatten()  # Convert y to 1D\n\n# Initial guess for beta\nbeta0 = np.zeros(X.shape[1])\n\n# Optimize the likelihood\nresult = minimize(poisson_regression_loglik, beta0, args=(Y, X), method='BFGS')\n\n# Extract MLE estimates and standard errors\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n# Present results in table\nresults_df = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": standard_errors\n}, index=X.columns)\n\nresults_df.round(4)\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nIntercept\n-0.5100\n0.1931\n\n\nC(region)[T.Northeast]\n0.0292\n0.0468\n\n\nC(region)[T.Northwest]\n-0.0176\n0.0572\n\n\nC(region)[T.South]\n0.0566\n0.0562\n\n\nC(region)[T.Southwest]\n0.0506\n0.0497\n\n\nage\n0.1487\n0.0145\n\n\nI(age ** 2)\n-0.0030\n0.0003\n\n\niscustomer\n0.2076\n0.0329\n\n\n\n\n\n\n\nBy including the intercept and relevant covariates in our design matrix and maximizing the log-likelihood using numerical optimization, we obtain statistically valid parameter estimates. These results allow us to rigorously assess the effect of Blueprinty’s software (and other variables) on the number of patents.\nTo verify the correctness of our manually optimized Poisson regression, we re-estimate the model using Python’s built-in Generalized Linear Model (GLM) implementation from the statsmodels package.\nThis model also uses the Poisson family with a log link, which matches the specification of our manual log-likelihood.\n\n\n\n\n\n\nResults using Python sm.GLM() function\n\n\n\n\n\n\nimport statsmodels.api as sm\n\n# Fit Poisson GLM with log link\nglm_model = sm.GLM(Y, X, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Display summary table\nprint(glm_results.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Wed, 21 May 2025   Deviance:                       2143.3\nTime:                        14:29:32   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n==========================================================================================\n                             coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------------------\nIntercept                 -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nC(region)[T.Northeast]     0.0292      0.044      0.669      0.504      -0.056       0.115\nC(region)[T.Northwest]    -0.0176      0.054     -0.327      0.744      -0.123       0.088\nC(region)[T.South]         0.0566      0.053      1.074      0.283      -0.047       0.160\nC(region)[T.Southwest]     0.0506      0.047      1.072      0.284      -0.042       0.143\nage                        0.1486      0.014     10.716      0.000       0.121       0.176\nI(age ** 2)               -0.0030      0.000    -11.513      0.000      -0.003      -0.002\niscustomer                 0.2076      0.031      6.719      0.000       0.147       0.268\n==========================================================================================\n\n\n\n\n\nInterpreting the Poisson Regression Results:\nFrom the comparison between our custom MLE output and statsmodels.GLM, we observe nearly identical coefficient estimates and standard errors, confirming that our implementation is correct.\nThe most notable results include:\n\niscustomer:\n\nCoefficient: 0.2076, Standard Error: 0.0329\nHighly statistically significant (p&lt;0.001)\nInterpretation: Firms using Blueprinty’s software are associated with a 23% increase in expected patent counts, all else equal. This is computed by exponentiating the coefficient (\\(\\exp (0.2076) \\approx 1.23\\))\n\nage:\n\nCoefficient: 0.1487, highly significant.\nSuggests that older firms tend to generate more patents.\n\nage ** 2:\n\nCoefficient: −0.0030, also highly significant.\nIndicates a diminishing return to age: very old firms may become less innovative or productive.\n\nRegion Dummie:\n\nNone of the region dummy variables are statistically significant at the 5% level.\nSuggests no strong regional effect on patent counts after controlling for other factors.\n\n\nTo summarize, the model provides strong evidence that Blueprinty’s software is associated with higher patent productivity. Also, the quadratic form of age shows that patent output rises with experience, but not indefinitely. Regional effects are minimal once other variables are included. These insights can help Blueprinty support its value proposition and target more impactful segments.\n\n\nBecause the coefficient from a Poisson regression model is not directly interpretable as a marginal effect, we estimate the effect of Blueprinty’s software by comparing counterfactual predictions.\nTo do so, we create two hypothetical versions of the dataset:\n\n\\(X_0\\): All firms set to iscustomer = 0 (non-users)\n\\(X_1\\): All firms set to iscustomer = 1 (Blueprinty users)\n\nUsing our fitted model, we compute the expected number of patents under both scenarios:\n\nX_0 = X.copy()\nX_1 = X.copy()\n\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n# Predict expected number of patents under both scenarios\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\n# Compute difference and average effect\ndiff = y_pred_1 - y_pred_0\naverage_effect = diff.mean()\n\nprint(f\"Average treatment effect of Blueprinty's software: {average_effect:.4f} additional patents\")\n\nAverage treatment effect of Blueprinty's software: 0.7928 additional patents\n\n\nThe estimated average treatment effect of Blueprinty’s software is 0.7928. This means that, on average, firms using Blueprinty’s software are predicted to obtain approximately 0.79 more patents over the 5-year period, holding all other firm characteristics constant.\n\n\n\nWe conclude that Blueprinty’s software is associated with a positive and meaningful increase in patent success:\n\nThe marginal effect of using the software is +0.79 patents on average.\nCombined with the earlier result that iscustomer was statistically significant in the regression, this provides strong evidence that Blueprinty’s product has a measurable and valuable effect on innovation outcomes."
  },
  {
    "objectID": "HW2/hw2_questions.html#blueprinty-case-study",
    "href": "HW2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nWe compare the number of patents held by firms that are Blueprinty customers vs. non-customers.\n\nimport pandas as pd\n\nblueprinty = pd.read_csv('blueprinty.csv')\nblueprinty.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nThe following histogram shows that Blueprinty customers (orange bars) tend to have slightly more patents than non-customers. The distribution is right-skewed for both groups, but the customer group has more firms with 5+ patents. Additionally, the mean number of patents for non-customers is 3.47 and for customers is 4.13.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Compute mean number of patents by customer status\nmean_patents = blueprinty.groupby(\"iscustomer\")[\"patents\"].mean()\n\n# Plot histogram of number of patents by customer status\nplt.figure(figsize=(10, 5))\nsns.histplot(data=blueprinty, x=\"patents\", hue=\"iscustomer\", multiple=\"dodge\", bins=20)\nplt.title(\"Histogram of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Is Customer\", labels=[\"Non-Customer\", \"Customer\"])\nplt.tight_layout()\n\nmean_patents\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\n\n\n\n\n\n\nObservation: There is a moderate difference in average patent count between customers and non-customers. This suggests that Blueprinty customers may be more successful in generating patents — though this raw difference may reflect underlying differences (e.g. age, region), not necessarily a causal effect of using the software.\nFor this reason, since Blueprinty customers are not selected at random, it may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Boxplot for firm age by customer status\nplt.figure(figsize=(8, 5))\nsns.boxplot(x=\"iscustomer\", y=\"age\", data=blueprinty)\nplt.title(\"Firm Age by Customer Status\")\nplt.xlabel(\"Is Customer\")\nplt.ylabel(\"Firm Age\")\nplt.xticks([0, 1], [\"Non-Customer\", \"Customer\"])\nplt.tight_layout()\n\n# Crosstab for region by customer status\nregion_customer_crosstab = pd.crosstab(blueprinty[\"region\"], blueprinty[\"iscustomer\"], normalize='index') * 100\nregion_customer_crosstab.round(1)\n\n\n\n\n\n\n\niscustomer\n0\n1\n\n\nregion\n\n\n\n\n\n\nMidwest\n83.5\n16.5\n\n\nNortheast\n45.4\n54.6\n\n\nNorthwest\n84.5\n15.5\n\n\nSouth\n81.7\n18.3\n\n\nSouthwest\n82.5\n17.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfter comparing regions and ages by customer status, we can see that, for the firm age, the boxplot shows that Blueprinty customers and non-customers have similar age distributions. However, customers have a slightly higher median age, and both groups show wide variation, with some firms as old as ~50 years. The overall distributions are fairly overlapping.\nOn the other hand, there are large differences in the regional distribution. Northeast has the highest customer share (54.6% of firms in this region are customers). In contrast, in regions like Midwest, Northwest, South, and Southwest, only ~15–18% of firms are customers.\nObservations: There is non-random customer selection: - Age is fairly balanced but might still matter. - Region is clearly associated with customer status, and firms in the Northeast are far more likely to be Blueprinty customers.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWe assume that each observation \\(Y_i\\) follows a Poisson distribution:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda)\n\\]\nThis means the probability mass function is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nAssuming the \\(Y_i\\) are i.i.d., the likelihood function is:\n\\[\n\\mathcal{L}(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{y_i}}{y_i!}\n\\]\nTaking the natural logarithm of the likelihood, we obtain the log-likelihood function:\n\\[\n\\ell(\\lambda) = \\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^n \\left[ y_i \\log(\\lambda) - \\lambda - \\log(y_i!) \\right]\n\\]\nNext, we define the log-likelihood for the Poisson distribution as a function of a single parameter, \\(\\lambda\\), and the observed data, \\(Y\\). This function is maximized to estimate the most likely value of \\(\\lambda\\) that could have generated the data:\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return np.inf  # avoid invalid log(λ) or exp(−λ)\n    Y = np.asarray(Y)\n    log_lik = np.sum(Y * np.log(lmbda) - lmbda - gammaln(Y + 1))\n    return log_lik  # negative for use with minimization\n\nWe then evaluate this function over a range of candidate values for \\(\\lambda\\), using the observed number of patents in the dataset. This helps us visually identify the value of \\(\\lambda\\) that maximizes the log-likelihood:\n\nY = blueprinty[\"patents\"].values\n\nlambda_vals = np.linspace(0.1, 10, 200)\nloglik_vals = [poisson_loglikelihood(lmb, Y) for lmb in lambda_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, color='darkblue')\nplt.title(\"Poisson Log-Likelihood vs. Lambda\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis plot reveals a clear peak in the log-likelihood function, indicating the most likely value of \\(\\lambda\\) given the observed number of patents. The shape confirms that the log-likelihood is concave, making optimization straightforward.\nTo confirm our visual result mathematically, we take the derivative of the log-likelihood function and solve for the value of \\(\\lambda\\) that maximizes it.\nStarting from the log-likelihood:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left[ y_i \\log(\\lambda) - \\lambda - \\log(y_i!) \\right]\n\\]\nTake the derivative with respect to \\(\\lambda\\):\n\\[\n\\frac{d\\ell}{d\\lambda} = \\frac{1}{\\lambda} \\sum_{i=1}^n y_i - n\n\\]\nSet the derivative to zero and solve:\n\\[\n\\begin{gather*}\n\\frac{1}{\\lambda} \\sum_{i=1}^n y_i - n = 0 \\\\\n\\sum_{i=1}^n y_i = n \\lambda \\\\\n\\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n y_i = \\bar{y}\n\\end{gather*}\n\\]\nConclusion: The MLE for \\(\\lambda\\) is simply the sample mean of the observed counts (\\(\\hat{\\lambda}_{\\text{MLE}} = \\bar{y}\\)). This aligns with our numerical result and reinforces the interpretation of \\(\\lambda\\) as the expected number of events (in this case, patents per firm).\nFinally, using scipy.optimize.minimize(), we numerically maximized the Poisson log-likelihood and obtained:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = 3.685\n\\]\nThis matches the sample mean of the observed patent counts, which aligns with our analytical derivation. The optimizer successfully found the value of \\(\\lambda\\) that maximizes the likelihood of observing the data under a Poisson model.\n\nfrom scipy.optimize import minimize\n\n# Define the negative log-likelihood for use with optimizer\ndef neg_poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return np.inf\n    return -poisson_loglikelihood(lmbda, Y)\n\n# Use the same Y from earlier\nY = blueprinty[\"patents\"].values\n\n# Use scipy to minimize the negative log-likelihood\nresult = minimize(fun=neg_poisson_loglikelihood, x0=[1.0], args=(Y,), bounds=[(1e-6, None)])\nlambda_mle = result.x[0]\nlambda_mle\n\n3.6846667021660804\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe now extend our Poisson likelihood to a regression setting, where the rate parameter \\(\\lambda_i\\) varies across observations. Specifically, we assume:\n\\[\n\\lambda_i = \\exp(X_i' \\beta)\n\\]\nThis ensures \\(\\lambda_i &gt; 0\\), as required by the Poisson distribution. The log-likelihood function becomes:\n\\[\n\\ell(\\beta) = \\sum_{i=1}^n \\left[ y_i (X_i' \\beta) - \\exp(X_i' \\beta) - \\log(y_i!) \\right]\n\\]\nWe implement this in Python by defining a function that takes the coefficient vector \\(\\beta\\), the observed count data \\(Y\\), and the covariate matrix \\(X\\):\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_loglik(beta, Y, X):\n    eta = X @ beta\n    eta = np.clip(eta, -20, 20)        # Prevent overflow in exp()\n    lam = np.exp(eta)\n    log_lik = np.sum(Y * eta - lam - gammaln(Y + 1))\n    return -log_lik                   # Return negative for minimizer\n\nTo estimate the Poisson regression model, we use Maximum Likelihood Estimation (MLE) via Python’s scipy.optimize.minimize() function. The goal is to obtain the MLE vector \\(\\hat{\\beta}\\) and compute the standard errors using the inverse of the Hessian matrix. We construct the covariate matrix \\(X\\) as follows:\n\nThe first column is a constant (1’s), enabling an intercept in the model.\nNext are: age, age², dummy variables for all but one region (reference category), and a binary indicator for whether the firm is a Blueprinty customer.\n\n\nimport patsy\n# Create design matrix with intercept, age, age², region dummies, and customer\ny, X = patsy.dmatrices('patents ~ age + I(age**2) + C(region) + iscustomer', data=blueprinty, return_type='dataframe')\nY = y.values.flatten()  # Convert y to 1D\n\n# Initial guess for beta\nbeta0 = np.zeros(X.shape[1])\n\n# Optimize the likelihood\nresult = minimize(poisson_regression_loglik, beta0, args=(Y, X), method='BFGS')\n\n# Extract MLE estimates and standard errors\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n# Present results in table\nresults_df = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": standard_errors\n}, index=X.columns)\n\nresults_df.round(4)\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nIntercept\n-0.5100\n0.1931\n\n\nC(region)[T.Northeast]\n0.0292\n0.0468\n\n\nC(region)[T.Northwest]\n-0.0176\n0.0572\n\n\nC(region)[T.South]\n0.0566\n0.0562\n\n\nC(region)[T.Southwest]\n0.0506\n0.0497\n\n\nage\n0.1487\n0.0145\n\n\nI(age ** 2)\n-0.0030\n0.0003\n\n\niscustomer\n0.2076\n0.0329\n\n\n\n\n\n\n\nBy including the intercept and relevant covariates in our design matrix and maximizing the log-likelihood using numerical optimization, we obtain statistically valid parameter estimates. These results allow us to rigorously assess the effect of Blueprinty’s software (and other variables) on the number of patents.\nTo verify the correctness of our manually optimized Poisson regression, we re-estimate the model using Python’s built-in Generalized Linear Model (GLM) implementation from the statsmodels package.\nThis model also uses the Poisson family with a log link, which matches the specification of our manual log-likelihood.\n\n\n\n\n\n\nResults using Python sm.GLM() function\n\n\n\n\n\n\nimport statsmodels.api as sm\n\n# Fit Poisson GLM with log link\nglm_model = sm.GLM(Y, X, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Display summary table\nprint(glm_results.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Wed, 21 May 2025   Deviance:                       2143.3\nTime:                        14:29:32   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n==========================================================================================\n                             coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------------------\nIntercept                 -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nC(region)[T.Northeast]     0.0292      0.044      0.669      0.504      -0.056       0.115\nC(region)[T.Northwest]    -0.0176      0.054     -0.327      0.744      -0.123       0.088\nC(region)[T.South]         0.0566      0.053      1.074      0.283      -0.047       0.160\nC(region)[T.Southwest]     0.0506      0.047      1.072      0.284      -0.042       0.143\nage                        0.1486      0.014     10.716      0.000       0.121       0.176\nI(age ** 2)               -0.0030      0.000    -11.513      0.000      -0.003      -0.002\niscustomer                 0.2076      0.031      6.719      0.000       0.147       0.268\n==========================================================================================\n\n\n\n\n\nInterpreting the Poisson Regression Results:\nFrom the comparison between our custom MLE output and statsmodels.GLM, we observe nearly identical coefficient estimates and standard errors, confirming that our implementation is correct.\nThe most notable results include:\n\niscustomer:\n\nCoefficient: 0.2076, Standard Error: 0.0329\nHighly statistically significant (p&lt;0.001)\nInterpretation: Firms using Blueprinty’s software are associated with a 23% increase in expected patent counts, all else equal. This is computed by exponentiating the coefficient (\\(\\exp (0.2076) \\approx 1.23\\))\n\nage:\n\nCoefficient: 0.1487, highly significant.\nSuggests that older firms tend to generate more patents.\n\nage ** 2:\n\nCoefficient: −0.0030, also highly significant.\nIndicates a diminishing return to age: very old firms may become less innovative or productive.\n\nRegion Dummie:\n\nNone of the region dummy variables are statistically significant at the 5% level.\nSuggests no strong regional effect on patent counts after controlling for other factors.\n\n\nTo summarize, the model provides strong evidence that Blueprinty’s software is associated with higher patent productivity. Also, the quadratic form of age shows that patent output rises with experience, but not indefinitely. Regional effects are minimal once other variables are included. These insights can help Blueprinty support its value proposition and target more impactful segments.\n\n\nBecause the coefficient from a Poisson regression model is not directly interpretable as a marginal effect, we estimate the effect of Blueprinty’s software by comparing counterfactual predictions.\nTo do so, we create two hypothetical versions of the dataset:\n\n\\(X_0\\): All firms set to iscustomer = 0 (non-users)\n\\(X_1\\): All firms set to iscustomer = 1 (Blueprinty users)\n\nUsing our fitted model, we compute the expected number of patents under both scenarios:\n\nX_0 = X.copy()\nX_1 = X.copy()\n\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n# Predict expected number of patents under both scenarios\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\n# Compute difference and average effect\ndiff = y_pred_1 - y_pred_0\naverage_effect = diff.mean()\n\nprint(f\"Average treatment effect of Blueprinty's software: {average_effect:.4f} additional patents\")\n\nAverage treatment effect of Blueprinty's software: 0.7928 additional patents\n\n\nThe estimated average treatment effect of Blueprinty’s software is 0.7928. This means that, on average, firms using Blueprinty’s software are predicted to obtain approximately 0.79 more patents over the 5-year period, holding all other firm characteristics constant.\n\n\n\nWe conclude that Blueprinty’s software is associated with a positive and meaningful increase in patent success:\n\nThe marginal effect of using the software is +0.79 patents on average.\nCombined with the earlier result that iscustomer was statistically significant in the regression, this provides strong evidence that Blueprinty’s product has a measurable and valuable effect on innovation outcomes."
  },
  {
    "objectID": "HW2/hw2_questions.html#airbnb-case-study",
    "href": "HW2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nIn this case study, we assume the number of reviews is a good proxy for the number of bookings, and we model the number of reviews using listing characteristics.\n\nData Preparation\nWe began by exploring the dataset and found several variables with missing values. To ensure a reliable analysis, we dropped rows missing any of the following variables:\n\nbathrooms\nbedrooms\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\n\nWe also converted categorical variables:\n\nroom_type: one-hot encoded with Entire home/apt as the baseline.\ninstant_bookable: mapped “t” to 1 and “f” to 0.\n\n\n\n\n\n\n\nPython Code for Data Preparation\n\n\n\n\n\n\nimport pandas as pd\nairbnb = pd.read_csv('airbnb.csv')\n\n# Drop rows with missing values in key predictors\nairbnb_clean = airbnb.dropna(subset=[\n    \"bathrooms\",\n    \"bedrooms\",\n    \"review_scores_cleanliness\",\n    \"review_scores_location\",\n    \"review_scores_value\"\n])\n\n# Convert categorical variables\nairbnb_clean[\"instant_bookable\"] = airbnb_clean[\"instant_bookable\"].map({\"t\": 1, \"f\": 0})\n\n# One-hot encode room_type, drop one category to avoid multicollinearity\nroom_dummies = pd.get_dummies(airbnb_clean[\"room_type\"], prefix=\"room\", drop_first=True)\n\n# Construct design matrix\nX = pd.concat([\n    airbnb_clean[[\n        \"price\",\n        \"bedrooms\",\n        \"bathrooms\",\n        \"review_scores_cleanliness\",\n        \"review_scores_location\",\n        \"review_scores_value\",\n        \"instant_bookable\"\n    ]],\n    room_dummies\n], axis=1)\n\n# Add intercept manually\nX.insert(0, \"intercept\", 1)\n\n# Define target variable\nY = airbnb_clean[\"number_of_reviews\"].values\n\n/tmp/ipykernel_37077/2696647610.py:14: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\n\n\nModel Specification\nWe modeled the number of reviews using a Poisson regression, where:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\text{where} \\quad \\lambda_i = \\exp(X_i' \\beta)\n\\]\nThe predictor variables \\(X_i\\) include:\n\nprice, bedrooms, bathrooms\nreview_scores_cleanliness, review_scores_location, review_scores_value\nroom_type dummies\ninstant_bookable\n\n\n\n\n\n\n\nPython Code for Poisson Regression\n\n\n\n\n\n\nimport statsmodels.api as sm\n\n# Ensure all columns in X are numeric\nX = X.astype(float)\n\n# Fit the Poisson regression model\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n# View the results\nprint(poisson_results.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                30160\nModel:                            GLM   Df Residuals:                    30150\nModel Family:                 Poisson   Df Model:                            9\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -5.2900e+05\nDate:                Wed, 21 May 2025   Deviance:                   9.3653e+05\nTime:                        14:29:32   Pearson chi2:                 1.41e+06\nNo. Iterations:                     6   Pseudo R-squ. (CS):             0.5649\nCovariance Type:            nonrobust                                         \n=============================================================================================\n                                coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nintercept                     3.5725      0.016    223.215      0.000       3.541       3.604\nprice                     -1.435e-05    8.3e-06     -1.729      0.084   -3.06e-05    1.92e-06\nbedrooms                      0.0749      0.002     37.698      0.000       0.071       0.079\nbathrooms                    -0.1240      0.004    -33.091      0.000      -0.131      -0.117\nreview_scores_cleanliness     0.1132      0.001     75.820      0.000       0.110       0.116\nreview_scores_location       -0.0768      0.002    -47.796      0.000      -0.080      -0.074\nreview_scores_value          -0.0915      0.002    -50.902      0.000      -0.095      -0.088\ninstant_bookable              0.3344      0.003    115.748      0.000       0.329       0.340\nroom_Private room            -0.0145      0.003     -5.310      0.000      -0.020      -0.009\nroom_Shared room             -0.2519      0.009    -29.229      0.000      -0.269      -0.235\n=============================================================================================\n\n\n\n\n\n\n\nModel Results\nWe estimated a Poisson regression model to understand how listing features are associated with the number of reviews (used here as a proxy for bookings). Coefficients are interpreted on a log scale, so we exponentiate them to obtain multiplicative effects on the expected count.\nCoefficient Interpretation:\n\nintercept:\n\n\\(\\beta_0 = 3.5725\\)\nRepresents the baseline log-expected number of reviews for an entire home/apt, not instantly bookable, with all numeric features equal to zero (not directly meaningful on its own).\n\nprice:\n\nThe coefficient is \\(\\beta = -1.435 \\times 10^{-5},\\ p = 0.084\\).\nNot statistically significant at the 5% level → price does not show a strong effect on review count when controlling for other variables.\n\nbedrooms:\n\nThe coefficient is \\(\\beta = 0.0749,\\ p &lt; 0.001\\).\n\\(\\exp(0.0749)\\approx 1.078\\)\nEach additional bedroom is associated with a 7.8% increase in expected reviews.\n\nbathrooms:\n\nThe coefficient is \\(\\beta = -0.1240,\\ p &lt; 0.001\\).\n\\(\\exp(−0.1240)\\approx 0.883\\)\nEach additional bathroom is associated with an 11.7% decrease in reviews, possibly reflecting shared accommodations.\n\nreview_scores_cleanliness:\n\nThe coefficient is \\(\\beta = 0.1132,\\ p &lt; 0.001\\).\n\\(\\exp(0.1132)\\approx 1.12\\)\nA 1-point increase in cleanliness score is associated with a 12% increase in reviews.\n\nreview_scores_location:\n\nThe coefficient is \\(\\beta = -0.0768,\\ p &lt; 0.001\\).\nCounterintuitive negative effect; may reflect selection bias or underlying location-based review dynamics.\n\nreview_scores_value:\n\nThe coefficient is \\(\\beta = −0.0915,\\ p &lt; 0.001\\).\n\\(\\exp(−0.0915)\\approx 0.91\\)\nHigher value scores are correlated with ~9% fewer reviews, which may not be causal.\n\ninstant_bookable:\n\nThe coefficient is \\(\\beta = 0.3344,\\ p &lt; 0.001\\).\n\\(\\exp(0.3344)\\approx 1.40\\)\nInstantly bookable listings receive ~40% more reviews on average.\n\nroom_Private room (vs. entire home):\n\nThe coefficient is \\(\\beta = −0.0145,\\ p &lt; 0.001\\).\n\\(\\exp(-0.0145) \\approx 0.986\\)\n~1.4% fewer reviews, a small but significant difference.\n\nroom_Shared room (vs. entire home):\n\nThe coefficient is \\(\\beta = −0.2519,\\ p &lt; 0.001\\).\n\\(\\exp(-0.2519) \\approx 0.777\\)\nShared rooms receive ~22% fewer reviews than entire homes.\n\n\n\n\nCONCLUSIONS\nOur findings suggest that:\n\nListings with more bedrooms and those that are instantly bookable receive significantly more reviews.\nShared accommodations and listings with higher “value” ratings receive fewer reviews, perhaps reflecting niche appeal or selection effects.\nThe model explains a considerable amount of variation in review counts (Pseudo R² = 0.5649), indicating strong predictive power."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\nSara Antentas Oliveras\nMay 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\nSara Antentas Oliveras\nMay 21, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "HW1/hw1_questions.html",
    "href": "HW1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn this study, the researchers tested whether offering a matching grant (where a donor pledges to match contributions) could increase charitable giving. The experiment involved sending over 50,000 direct mail solicitations to previous donors of a liberal nonprofit organization. Individuals were randomly assigned to either a control group or to one of several treatment groups. The treatment letters announced that a “concerned fellow member” would match their donation at varying ratios: 1:1, 2:1, or 3:1, with caps on the total matched amount. Additionally, letters varied in the suggested donation amount and the stated match ceiling ($25,000, $50,000, $100,000, or not mentioned). This design allowed the authors to isolate the effects of price, messaging, and donor context on charitable behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "HW1/hw1_questions.html#introduction",
    "href": "HW1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn this study, the researchers tested whether offering a matching grant (where a donor pledges to match contributions) could increase charitable giving. The experiment involved sending over 50,000 direct mail solicitations to previous donors of a liberal nonprofit organization. Individuals were randomly assigned to either a control group or to one of several treatment groups. The treatment letters announced that a “concerned fellow member” would match their donation at varying ratios: 1:1, 2:1, or 3:1, with caps on the total matched amount. Additionally, letters varied in the suggested donation amount and the stated match ceiling ($25,000, $50,000, $100,000, or not mentioned). This design allowed the authors to isolate the effects of price, messaging, and donor context on charitable behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "HW1/hw1_questions.html#data",
    "href": "HW1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nWe use the dataset karlan_list_2007.dta, which contains 50,083 observations, each corresponding to a past donor who received one of several types of fundraising letters. The dataset is provided in Stata (.dta) format and can be read into Python using the pandas library:\n\nimport pandas as pd\n\n# Load the Stata data\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Preview the data\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\nThe dataset includes information about:\n\nTreatment conditions: such as the match ratio (ratio, ratio2, ratio3), match cap (size25, size50, size100), and suggested donation (askd1, askd2, askd3).\nDonation outcomes: including whether the individual donated (gave) and how much (amount).\nDonor history: such as the number of past donations (freq), years since first donation (years), and highest previous contribution (hpa).\nDemographics & geography: gender (female), political environment (red0, blue0), and zip-code-level demographics like income and education.\n\nThis rich structure allows us to explore how matching grants and suggested donation levels affect giving behavior.\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nTo test whether several pre-treatment characteristics differ significantly between the treatment and control groups, I used both manual t-tests and linear regressions, where each variable was regressed on the treatment assignment.\nThe results are shown below:\n\n\n\n\n\n\nPython Code to Compute Balance Tests\n\n\n\n\n\n\nimport pandas as pd\nimport statsmodels.formula.api as smf\nfrom scipy.stats import t as t_dist\n\n# Variables to test\nvariables = ['mrm2', 'freq', 'hpa', 'female']\n\n# Collect results\nresults = []\n\nfor var in variables:\n    # Split by treatment group\n    treated = df[df[\"treatment\"] == 1][var].dropna()\n    control = df[df[\"treatment\"] == 0][var].dropna()\n\n    # Compute sample stats\n    x_t, x_c = treated.mean(), control.mean()\n    s2_t, s2_c = treated.var(), control.var()\n    n_t, n_c = treated.shape[0], control.shape[0]\n\n    # Manual t-test\n    t_stat = (x_t - x_c) / ((s2_t / n_t + s2_c / n_c) ** 0.5)\n    df_num = (s2_t / n_t + s2_c / n_c) ** 2\n    df_denom = ((s2_t / n_t) ** 2) / (n_t - 1) + ((s2_c / n_c) ** 2) / (n_c - 1)\n    dof = df_num / df_denom\n    p_val_ttest = 2 * t_dist.sf(abs(t_stat), dof)\n\n    # Linear regression\n    formula = f\"{var} ~ treatment\"\n    model = smf.ols(formula, data=df).fit()\n    coef = model.params['treatment']\n    p_val_reg = model.pvalues['treatment']\n\n    # Store results\n    results.append({\n        'Variable': var,\n        'Mean (Treatment)': round(x_t, 3),\n        'Mean (Control)': round(x_c, 3),\n        't-stat (manual)': round(t_stat, 3),\n        'p-value (manual)': round(p_val_ttest, 3),\n        'Coef (regression)': round(coef, 3),\n        'p-value (regression)': round(p_val_reg, 3)\n    })\n\n# Summary table\nresults_df = pd.DataFrame(results)\n\n\n\n\n\nSummary Table\n\nresults_df\n\n\n\n\n\n\n\n\nVariable\nMean (Treatment)\nMean (Control)\nt-stat (manual)\np-value (manual)\nCoef (regression)\np-value (regression)\n\n\n\n\n0\nmrm2\n13.012\n12.998000\n0.120\n0.905\n0.014\n0.905\n\n\n1\nfreq\n8.035\n8.047000\n-0.111\n0.912\n-0.012\n0.912\n\n\n2\nhpa\n59.597\n58.959999\n0.970\n0.332\n0.637\n0.345\n\n\n3\nfemale\n0.275\n0.283000\n-1.754\n0.080\n-0.008\n0.079\n\n\n\n\n\n\n\nIn all cases, the p-values are above 0.05, indicating that we fail to reject the null hypothesis of equal means between treatment and control groups. The results from the manual t-tests and regression models are nearly identical, as expected.\nThis confirms that random assignment was successful — the treatment and control groups are statistically balanced on observed characteristics. These results mirror Table 1 in Karlan & List (2007), where the treatment and control groups show nearly identical averages across key baseline variables. This similarity supports the idea that the randomization worked as intended, meaning any differences in outcomes we observe later can more confidently be attributed to the treatment, not to pre-existing differences between the groups."
  },
  {
    "objectID": "HW1/hw1_questions.html#experimental-results",
    "href": "HW1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nI compare donation rates between the treatment and control groups. The plot below shows that 2.2% of individuals in the treatment group donated, compared to 1.8% in the control group. This visual difference suggests a potential treatment effect.\n\nimport matplotlib.pyplot as plt\n\ndonation_rates = df.groupby(\"treatment\")[\"gave\"].mean()\ndonation_rates.index = [\"Control\", \"Treatment\"]\n\nplt.bar(donation_rates.index, donation_rates.values)\nplt.ylabel(\"Proportion who donated\")\nplt.title(\"Donation Rates by Group\")\nplt.ylim(0, 0.03)\nplt.show()\n\n\n\n\n\n\n\n\nThen, to test whether offering a matching donation increases the likelihood that someone donates, I compare the donation behavior between the treatment and control groups using both a t-test and a bivariate linear regression.\n\n# Manual t-test using class formula\ntreated = df[df[\"treatment\"] == 1][\"gave\"].dropna()\ncontrol = df[df[\"treatment\"] == 0][\"gave\"].dropna()\n\nx_t, x_c = treated.mean(), control.mean()\ns2_t, s2_c = treated.var(), control.var()\nn_t, n_c = len(treated), len(control)\n\nt_stat = (x_t - x_c) / ((s2_t/n_t + s2_c/n_c) ** 0.5)\n\n# Welch–Satterthwaite degrees of freedom\ndf_num = (s2_t/n_t + s2_c/n_c)**2\ndf_denom = ((s2_t/n_t)**2)/(n_t - 1) + ((s2_c/n_c)**2)/(n_c - 1)\ndof = df_num / df_denom\n\n# Two-sided p-value\np_val = 2 * t_dist.sf(abs(t_stat), dof)\n\nprint(f\"Manual t-stat: {t_stat:.3f}\")\nprint(f\"Manual p-value: {p_val:.4f}\")\n\nManual t-stat: 3.209\nManual p-value: 0.0013\n\n\n\n# Linear regression\nmodel_lin = smf.ols(\"gave ~ treatment\", data=df).fit()\n# print(model_lin.summary())\n\nRegression Output Summary:\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nt-value\np-value\n95% CI\n\n\n\n\nIntercept\n0.0179\n0.001\n16.225\n0.000\n[0.016, 0.020]\n\n\nTreatment\n0.0042\n0.001\n3.101\n0.002\n[0.002, 0.007]\n\n\n\nThe t-test reveals a statistically significant difference in donation rates: the treatment group is more likely to give than the control group. This finding is confirmed by the regression, where the treatment coefficient is positive and significant, indicating that receiving a letter with a matching offer caused an increase in donations.\nThese results align closely with Table 2A, Panel A from Karlan & List (2007), which shows a response rate of 1.8% for the control group and 2.2% for the treatment group. The increase may seem small in absolute terms, but it is meaningful given the scale of the experiment and is statistically distinguishable from zero.\nSo, what does this tell us about human behavior? People are more likely to act generously when they feel their contribution is amplified. The idea that a matching donor will “double” or “triple” their impact likely makes the donation feel more meaningful. Even if the match ratio is relatively small, this added sense of social proof, urgency, or amplified impact nudges people to take action. In short, while most people didn’t give, those who were offered a matching donation were noticeably more likely to say yes. This demonstrates how small design changes in fundraising campaigns can have meaningful effects on human behavior.\nTo further analyze the effect of being offered a matching donation on the likelihood of giving, I estimate a probit regression, where the outcome variable is whether a charitable donation is made (gave), and the explanatory variable is treatment assignment.\n\n# Probit regression\nmodel_probit = smf.probit(\"gave ~ treatment\", data=df).fit()\n#print(model_probit.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\nRegression Output Summary:\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nz-value\np-value\n95% CI\n\n\n\n\nIntercept\n−2.1001\n0.023\n−90.073\n0.000\n[−2.146, −2.054]\n\n\nTreatment\n0.0868\n0.028\n3.113\n0.002\n[0.032, 0.141]\n\n\n\nThis result is statistically significant at the 1% level, and the sign and magnitude match closely with Table 3, Column 1 in the paper, where the authors report a coefficient of 0.086 with a standard error of 0.028.\nAlthough probit coefficients aren’t directly interpretable in terms of probability changes, the positive and significant result indicates that being in the treatment group increases the probability of donating.\nThis confirms what we see in the t-test and linear regression: offering a match increases donations, even when using a model tailored for binary outcomes.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nTo investigate whether larger match ratios (e.g., 2:1 or 3:1) encourage more people to donate than the 1:1 match, I run a series of t-tests comparing donation rates among the different match groups (all within the treatment group).\n\n\n\n\n\n\nPython Code to Compute Different t-tests\n\n\n\n\n\n\n# Subset to treatment group\ntreat_group = df[df[\"treatment\"] == 1]\n\n# Example: compare 2:1 vs 1:1\nx2 = treat_group[treat_group[\"ratio2\"] == 1][\"gave\"]\nx1 = treat_group[treat_group[\"ratio\"] == 1][\"gave\"]\n\nx_t, x_c = x2.mean(), x1.mean()\ns2_t, s2_c = x2.var(), x1.var()\nn_t, n_c = len(x2), len(x1)\n\nt_stat = (x_t - x_c) / ((s2_t/n_t + s2_c/n_c) ** 0.5)\n\n# Welch–Satterthwaite degrees of freedom\ndf_num = (s2_t/n_t + s2_c/n_c)**2\ndf_denom = ((s2_t/n_t)**2)/(n_t - 1) + ((s2_c/n_c)**2)/(n_c - 1)\ndof = df_num / df_denom\n\n# Two-sided p-value\np_val = 2 * t_dist.sf(abs(t_stat), dof)\n\n\n\n\n\nprint(f\"Manual t-stat: {t_stat:.3f}\")\nprint(f\"Manual p-value: {p_val:.4f}\")\n\nManual t-stat: 0.965\nManual p-value: 0.3345\n\n\nThe results are clear: there is no statistically significant differences in response rates between the 1:1, 2:1, and 3:1 match ratios. All t-statistics are approximately 0.965, and all p-values are around 0.335, well above conventional significance thresholds.\nThese results replicate the authors’ conclusion in the paper (see page 8), where they write: “…we do not find systematic patterns for the interaction effects.”\nIn other words, while offering a match increases donation rates (compared to no match), increasing the match ratio beyond 1:1 does not meaningfully boost donations further.\nTo complement the t-tests, I also run a regression on the subsample of individuals who received a matching donation. The goal is to estimate how donation likelihood varies across different match ratios: 1:1 (baseline), 2:1, and 3:1. I use an OLS regression where the dependent variable is gave, and the explanatory variables are indicator variables for the match ratios (ratio2, ratio3). The 1:1 match group serves as the reference category.\n\n# Run regression with 1:1 as baseline (ratio1 is omitted)\nmodel = smf.ols(\"gave ~ ratio2 + ratio3\", data=treat_group).fit()\n#print(model.summary())\n\nRegression Output Summary:\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nt-value\np-value\n95% CI\n\n\n\n\nIntercept\n0.0207\n0.001\n14.912\n0.000\n[0.018, 0.023]\n\n\nratio2 (2:1)\n0.0019\n0.002\n0.958\n0.338\n[−0.002, 0.006]\n\n\nratio3 (3:1)\n0.0020\n0.002\n1.008\n0.313\n[−0.002, 0.006]\n\n\n\nThis regression confirms the earlier t-test results: neither a 2:1 nor a 3:1 match rate leeds to a statistically significant increase in giving compared to a 1:1 match. The coefficients are small (~0.002) and not statistically significant (p &gt; 0.3). This suggests that offering a match matters more than the size of the match. Larger match offers (e.g., 3:1) do not appear to yield meaningful gains over a simple 1:1 offer.\nTo further assess whether larger match ratios led to more donations, I calculate the difference in donation rates between:\n\n2:1 vs 1:1\n3:1 vs 2:1\n3:1 vs 1:1\n\nFrom the treatment group, I compute the mean of gave for each match ratio group:\n\n\n\n\n\n\nRaw response rates by match ratio\n\n\n\n\n\n\n# Create match ratio group labels\ndef get_ratio(row):\n    if row[\"ratio2\"] == 1:\n        return \"2:1\"\n    elif row[\"ratio3\"] == 1:\n        return \"3:1\"\n    else:\n        return \"1:1\"\n\n# Safely add the new column\ntreat_group.loc[:, \"ratio_group\"] = treat_group.apply(get_ratio, axis=1)\n\n# Calculate mean donation rate by group\ndonation_rates = treat_group.groupby(\"ratio_group\")[\"gave\"].mean()\n\n/tmp/ipykernel_42512/2459960996.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\n\nprint(\"Raw Donation Rates by Match Ratio:\\n\", donation_rates)\n\nRaw Donation Rates by Match Ratio:\n ratio_group\n1:1    0.020749\n2:1    0.022633\n3:1    0.022733\nName: gave, dtype: float64\n\n\nSo:\n\n2:1 − 1:1 = 0.19 percentage points\n3:1 − 1:1 = 0.20 percentage points\n3:1 − 2:1 = 0.01 percentage points\n\nThese differences are very small.\nAlso, from the regression output, the coefficients for ratio2 and ratio3 are 0.0019 and 0.0020, respectively. These values again show that the 2:1 match group donated ~0.19 percentage points more than the 1:1 group; the 3:1 match group donated ~0.20 points more than the 1:1 group; and the difference between 3:1 and 2:1 is basically zero (0.0020 − 0.0019 = 0.0001).\nBoth the raw means and the regression results tell the same story: increasing the match ratio from 1:1 to 2:1 or 3:1 leads to slightly higher donation rates, but the increases are small and not statistically significant.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nFirst, to analyze whether the treatment (being offered a matching donation) influenced not just whether someone gave, but how much they gave, I conduct a t-test and a bivariate linear regression on the full sample (including individuals who gave $0).\n\n\n\n\n\n\nCompare donation amounts across all individuals\n\n\n\n\n\n\n# (most people donated $0)\nfrom scipy.stats import ttest_ind\ntreated = df[df[\"treatment\"] == 1][\"amount\"].dropna()\ncontrol = df[df[\"treatment\"] == 0][\"amount\"].dropna()\n\n# T-test\nt_stat, p_val = ttest_ind(treated, control, equal_var=False)\n\n# Linear regression (everyone)\nmodel_all = smf.ols(\"amount ~ treatment\", data=df).fit()\n\n\n\n\n\nprint(f\"T-test: t = {t_stat:.3f}, p = {p_val:.4f}\")\n#print(model_all.summary())\n\nT-test: t = 1.918, p = 0.0551\n\n\nRegression Output Summary:\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nt-value\np-value\n95% CI\n\n\n\n\nIntercept\n0.8133\n0.067\n12.063\n0.000\n[0.681, 0.945]\n\n\nTreatment\n0.1536\n0.083\n1.861\n0.063\n[−0.008, 0.315]\n\n\n\nThe t-test returns a t-statistic of 1.918 with a p-value of 0.0551, and the linear regression produces a treatment coefficient of 0.1536 (p = 0.063). These values are not statistically significant at the 5% level, but they are marginally close.\nThis analysis suggests that on average, individuals in the treatment group gave slightly more than those in the control group. However, this difference is not strong enough to conclude that the treatment definitively increased average donation size.\nImportantly, since the majority of individuals donated $0, this result primarily reflects differences in the likelihood of giving, not in the size of donations among actual donors. In other words, what we’re seeing is likely driven by extensive margin behavior — more people gave — rather than people giving more.\nTo truly assess whether the treatment affected the size of donations, we need to focus on those who actually gave, which is what we’ll do next.\nTo isolate the effect of the treatment on the amount donated, I restricted the analysis to only those individuals who actually gave. This lets us explore whether the matching offer not only encouraged more people to give, but also led them to give more money once they decided to donate.\nI ran a t-test and a linear regression of amount on treatment, using this restricted sample of donors.\n\n\n\n\n\n\nCompare donation amounts filtering only donors\n\n\n\n\n\n\n# Filter only donors (those who gave a positive amount)\ndonors = df[df[\"gave\"] == 1]\n\n# Compare mean donation amounts between treatment & control among donors\ntreated_donors = donors[donors[\"treatment\"] == 1][\"amount\"]\ncontrol_donors = donors[donors[\"treatment\"] == 0][\"amount\"]\n\nt_stat, p_val = ttest_ind(treated_donors, control_donors, equal_var=False)\n\n# Run regression: amount ~ treatment\nmodel_donors = smf.ols(\"amount ~ treatment\", data=donors).fit()\n\n\n\n\n\nprint(f\"T-test (donors only): t = {t_stat:.3f}, p = {p_val:.4f}\")\n#print(model_donors.summary())\n\nT-test (donors only): t = -0.585, p = 0.5590\n\n\nRegression Output Summary:\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nt-value\np-value\n95% CI\n\n\n\n\nIntercept\n45.5403\n2.423\n18.792\n0.000\n[40.785, 50.296]\n\n\nTreatment\n−1.6684\n2.872\n−0.581\n0.561\n[−7.305, 3.968]\n\n\n\nThese results indicate that, among donors, those who received the match offer actually gave slightly less than those in the control group, though the difference is not statistically significant.\nThis analysis suggests that the match offer was effective at getting more people to donate, but it did not increase the size of donations among those who were already willing to give. If anything, the point estimate implies the opposite, but the evidence is not strong enough to draw a firm conclusion.\nIn other words, the treatment worked on the extensive margin (getting more people to give), but not on the intensive margin (getting donors to give more).\nThe treatment coefficient doesn’t exactly have a causal interpretation, because, while the original experiment involved random assignment, this analysis conditions on a post-treatment variable (gave == 1), which may itself have been influenced by the treatment. This introduces selection bias, because the people who chose to give in each group may differ systematically.\nAs a result, the treatment coefficient here is descriptive (it tells us how much donors gave in each group) but it does not provide a valid causal estimate of how treatment affected donation size.\nFinally, to visualize how donation amounts differed between groups, I created two histograms (one for the treatment group and one for the control group) including only individuals who actually made a donation. A red vertical dashed line in each plot represents the average donation amount for that group.\n\nimport matplotlib.pyplot as plt\n\n# Subset only those who donated\ndonors = df[df[\"gave\"] == 1]\n\n# Split by treatment\ntreatment_donors = donors[donors[\"treatment\"] == 1][\"amount\"]\ncontrol_donors = donors[donors[\"treatment\"] == 0][\"amount\"]\n\n# Compute means\nmean_treat = treatment_donors.mean()\nmean_control = control_donors.mean()\n\n# Plot histograms\nfig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n\n# Control group plot\naxes[0].hist(control_donors, bins=30, color='skyblue', edgecolor='black')\naxes[0].axvline(mean_control, color='red', linestyle='dashed', linewidth=2, label=f\"Mean = {mean_control:.2f}\")\naxes[0].set_title(\"Control Group\")\naxes[0].set_xlabel(\"Donation Amount\")\naxes[0].set_ylabel(\"Frequency\")\naxes[0].legend()\n\n# Treatment group plot\naxes[1].hist(treatment_donors, bins=30, color='lightgreen', edgecolor='black')\naxes[1].axvline(mean_treat, color='red', linestyle='dashed', linewidth=2, label=f\"Mean = {mean_treat:.2f}\")\naxes[1].set_title(\"Treatment Group\")\naxes[1].set_xlabel(\"Donation Amount\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nObservations from the Plot\n\nThe distribution shapes are fairly similar across both groups: most donations fall below $100, with a few outliers in the $200–$400 range.\nThe average donation in the control group was $45.54, while the average in the treatment group was $43.87.\nDespite offering a matching donation, the treatment group did not donate more on average. In fact, they gave slightly less (though not significantly).\n\nThese findings match what we saw in the regression and t-test: while the treatment increased the likelihood of giving, it did not lead to larger donations among those who gave."
  },
  {
    "objectID": "HW1/hw1_questions.html#simulation-experiment",
    "href": "HW1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nTo illustrate the Law of Large Numbers, I simulate donation behavior for two groups:\n\nControl group: Bernoulli trials with probability p=0.018 (1.8% chance of donating)\nTreatment group: Bernoulli trials with probability p=0.022 (2.2% chance of donating)\n\nAt each iteration, I simulate one draw from each distribution and record the difference (treatment − control). I repeat this 10,000 times, then plotted the cumulative average of these differences across iterations. The red dashed line in the plot shows the true difference in donation probabilities: True difference=0.022−0.018=0.004\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Simulation parameters\nn = 10000\np_control = 0.018\np_treatment = 0.022\n\n# Simulate donations (1 = gave, 0 = didn't)\ncontrol_draws = np.random.binomial(1, p_control, n)\ntreatment_draws = np.random.binomial(1, p_treatment, n)\n\n# Compute stepwise differences in donation outcomes\ndiffs = treatment_draws - control_draws\n\n# Cumulative average of differences\ncumulative_avg_diff = np.cumsum(diffs) / np.arange(1, n+1)\n\n# Plot\nplt.figure(figsize=(10, 4))\nplt.plot(cumulative_avg_diff, label='Cumulative Average Difference', color='blue')\nplt.axhline(p_treatment - p_control, color='red', linestyle='dashed', label=f\"True Diff = {p_treatment - p_control:.004f}\")\nplt.title(\"Law of Large Numbers: Cumulative Average Difference\")\nplt.xlabel(\"Simulation Iteration\")\nplt.ylabel(\"Average Difference in Donation Rates\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nIn this plot we can see taht, initially, the cumulative average fluctuates heavily (the early estimates are noisy due to small sample sizes). But as the number of simulated draws increases, the average converges toward the true value, stabilizing close to 0.004.\nThis is a classic visual demonstration of the Law of Large Numbers, which tells us that, as the number of observations grows, the sample mean converges to the population mean.\n\n\nCentral Limit Theorem\nTo visualize the Central Limit Theorem, I simulate the average difference in donation rates between treatment and control groups using four different sample sizes: 50, 200, 500, and 1000. For each sample size, I draw samples of size n from Bernoulli distributions with a probabilities p=0.018 and p=0.022, for the control group and treatment group respectively. Then, I compute the difference in sample means between the groups, and I repeat this process 1000 times, generating 1000 average differences for each sample size. I then plot a histogram of those 1000 differences.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed\nnp.random.seed(42)\n\n# Parameters\np_control = 0.018\np_treatment = 0.022\nsample_sizes = [50, 200, 500, 1000]\nnum_simulations = 1000\n\n# Create subplots\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    avg_diffs = []\n\n    for _ in range(num_simulations):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        avg_diffs.append(diff)\n\n    # Plot histogram\n    axes[i].hist(avg_diffs, bins=30, color='lightblue', edgecolor='black')\n    axes[i].axvline(x=0, color='red', linestyle='dashed', label='Zero')\n    axes[i].axvline(x=np.mean(avg_diffs), color='black', linestyle='dotted', label='Sample Mean')\n    axes[i].set_title(f\"Sample Size = {n}\")\n    axes[i].set_xlabel(\"Avg. Difference in Donation Rates\")\n    axes[i].set_ylabel(\"Frequency\")\n    axes[i].legend()\n\nplt.suptitle(\"Central Limit Theorem: Distribution of Average Differences\", fontsize=14)\nplt.tight_layout()\nplt.subplots_adjust(top=0.9)\nplt.show()\n\n\n\n\n\n\n\n\n\nWhat the Plots Show\n\nSample Size = 50: The distribution is wide and somewhat irregular. It includes zero near the center, suggesting that with small samples, random noise can easily obscure the true effect.\nSample Size = 200: The distribution becomes narrower and more symmetric. The average difference starts to center around the true difference (~0.004).\nSample Size = 500: The shape becomes more bell-like, and zero begins to move toward the edge of the distribution.\nSample Size = 1000: The distribution is clearly approximately normal and tightly concentrated around the true average difference. Zero is now well into the tail, indicating that it’s increasingly unlikely the observed difference is due to chance.\n\nThis simulation confirms the Central Limit Theorem in practice: As sample size increases, the distribution of the sample mean becomes approximately normal, regardless of the underlying distribution, and its standard error decreases.\nIn the context of the donation experiment, it shows that with small samples, detecting a small effect like a 0.4 percentage point increase is hard and noisy. But as the sample size grows, the signal becomes more visible, and zero becomes a less likely explanation for the difference.\nThis reinforces the value of large samples in experiments, since they provide more precise, reliable estimates of treatment effects."
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn this study, the researchers tested whether offering a matching grant (where a donor pledges to match contributions) could increase charitable giving. The experiment involved sending over 50,000 direct mail solicitations to previous donors of a liberal nonprofit organization. Individuals were randomly assigned to either a control group or to one of several treatment groups. The treatment letters announced that a “concerned fellow member” would match their donation at varying ratios: 1:1, 2:1, or 3:1, with caps on the total matched amount. Additionally, letters varied in the suggested donation amount and the stated match ceiling ($25,000, $50,000, $100,000, or not mentioned). This design allowed the authors to isolate the effects of price, messaging, and donor context on charitable behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/index.html#introduction",
    "href": "blog/project1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn this study, the researchers tested whether offering a matching grant (where a donor pledges to match contributions) could increase charitable giving. The experiment involved sending over 50,000 direct mail solicitations to previous donors of a liberal nonprofit organization. Individuals were randomly assigned to either a control group or to one of several treatment groups. The treatment letters announced that a “concerned fellow member” would match their donation at varying ratios: 1:1, 2:1, or 3:1, with caps on the total matched amount. Additionally, letters varied in the suggested donation amount and the stated match ceiling ($25,000, $50,000, $100,000, or not mentioned). This design allowed the authors to isolate the effects of price, messaging, and donor context on charitable behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/index.html#data",
    "href": "blog/project1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nWe use the dataset karlan_list_2007.dta, which contains 50,083 observations, each corresponding to a past donor who received one of several types of fundraising letters. The dataset is provided in Stata (.dta) format and can be read into Python using the pandas library:\n\nimport pandas as pd\n\n# Load the Stata data\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Preview the data\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\nThe dataset includes information about:\n\nTreatment conditions: such as the match ratio (ratio, ratio2, ratio3), match cap (size25, size50, size100), and suggested donation (askd1, askd2, askd3).\nDonation outcomes: including whether the individual donated (gave) and how much (amount).\nDonor history: such as the number of past donations (freq), years since first donation (years), and highest previous contribution (hpa).\nDemographics & geography: gender (female), political environment (red0, blue0), and zip-code-level demographics like income and education.\n\nThis rich structure allows us to explore how matching grants and suggested donation levels affect giving behavior.\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nTo test whether several pre-treatment characteristics differ significantly between the treatment and control groups, I used both manual t-tests and linear regressions, where each variable was regressed on the treatment assignment.\nThe results are shown below:\n\n\n\n\n\n\nPython Code to Compute Balance Tests\n\n\n\n\n\n\nimport pandas as pd\nimport statsmodels.formula.api as smf\nfrom scipy.stats import t as t_dist\n\n# Variables to test\nvariables = ['mrm2', 'freq', 'hpa', 'female']\n\n# Collect results\nresults = []\n\nfor var in variables:\n    # Split by treatment group\n    treated = df[df[\"treatment\"] == 1][var].dropna()\n    control = df[df[\"treatment\"] == 0][var].dropna()\n\n    # Compute sample stats\n    x_t, x_c = treated.mean(), control.mean()\n    s2_t, s2_c = treated.var(), control.var()\n    n_t, n_c = treated.shape[0], control.shape[0]\n\n    # Manual t-test\n    t_stat = (x_t - x_c) / ((s2_t / n_t + s2_c / n_c) ** 0.5)\n    df_num = (s2_t / n_t + s2_c / n_c) ** 2\n    df_denom = ((s2_t / n_t) ** 2) / (n_t - 1) + ((s2_c / n_c) ** 2) / (n_c - 1)\n    dof = df_num / df_denom\n    p_val_ttest = 2 * t_dist.sf(abs(t_stat), dof)\n\n    # Linear regression\n    formula = f\"{var} ~ treatment\"\n    model = smf.ols(formula, data=df).fit()\n    coef = model.params['treatment']\n    p_val_reg = model.pvalues['treatment']\n\n    # Store results\n    results.append({\n        'Variable': var,\n        'Mean (Treatment)': round(x_t, 3),\n        'Mean (Control)': round(x_c, 3),\n        't-stat (manual)': round(t_stat, 3),\n        'p-value (manual)': round(p_val_ttest, 3),\n        'Coef (regression)': round(coef, 3),\n        'p-value (regression)': round(p_val_reg, 3)\n    })\n\n# Summary table\nresults_df = pd.DataFrame(results)\n\n\n\n\n\nSummary Table\n\nresults_df\n\n\n\n\n\n\n\n\nVariable\nMean (Treatment)\nMean (Control)\nt-stat (manual)\np-value (manual)\nCoef (regression)\np-value (regression)\n\n\n\n\n0\nmrm2\n13.012\n12.998000\n0.120\n0.905\n0.014\n0.905\n\n\n1\nfreq\n8.035\n8.047000\n-0.111\n0.912\n-0.012\n0.912\n\n\n2\nhpa\n59.597\n58.959999\n0.970\n0.332\n0.637\n0.345\n\n\n3\nfemale\n0.275\n0.283000\n-1.754\n0.080\n-0.008\n0.079\n\n\n\n\n\n\n\nIn all cases, the p-values are above 0.05, indicating that we fail to reject the null hypothesis of equal means between treatment and control groups. The results from the manual t-tests and regression models are nearly identical, as expected.\nThis confirms that random assignment was successful — the treatment and control groups are statistically balanced on observed characteristics. These results mirror Table 1 in Karlan & List (2007), where the treatment and control groups show nearly identical averages across key baseline variables. This similarity supports the idea that the randomization worked as intended, meaning any differences in outcomes we observe later can more confidently be attributed to the treatment, not to pre-existing differences between the groups."
  },
  {
    "objectID": "blog/project1/index.html#experimental-results",
    "href": "blog/project1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nI compare donation rates between the treatment and control groups. The plot below shows that 2.2% of individuals in the treatment group donated, compared to 1.8% in the control group. This visual difference suggests a potential treatment effect.\n\nimport matplotlib.pyplot as plt\n\ndonation_rates = df.groupby(\"treatment\")[\"gave\"].mean()\ndonation_rates.index = [\"Control\", \"Treatment\"]\n\nplt.bar(donation_rates.index, donation_rates.values)\nplt.ylabel(\"Proportion who donated\")\nplt.title(\"Donation Rates by Group\")\nplt.ylim(0, 0.03)\nplt.show()\n\n\n\n\n\n\n\n\nThen, to test whether offering a matching donation increases the likelihood that someone donates, I compare the donation behavior between the treatment and control groups using both a t-test and a bivariate linear regression.\n\n# Manual t-test using class formula\ntreated = df[df[\"treatment\"] == 1][\"gave\"].dropna()\ncontrol = df[df[\"treatment\"] == 0][\"gave\"].dropna()\n\nx_t, x_c = treated.mean(), control.mean()\ns2_t, s2_c = treated.var(), control.var()\nn_t, n_c = len(treated), len(control)\n\nt_stat = (x_t - x_c) / ((s2_t/n_t + s2_c/n_c) ** 0.5)\n\n# Welch–Satterthwaite degrees of freedom\ndf_num = (s2_t/n_t + s2_c/n_c)**2\ndf_denom = ((s2_t/n_t)**2)/(n_t - 1) + ((s2_c/n_c)**2)/(n_c - 1)\ndof = df_num / df_denom\n\n# Two-sided p-value\np_val = 2 * t_dist.sf(abs(t_stat), dof)\n\nprint(f\"Manual t-stat: {t_stat:.3f}\")\nprint(f\"Manual p-value: {p_val:.4f}\")\n\nManual t-stat: 3.209\nManual p-value: 0.0013\n\n\n\n# Linear regression\nmodel_lin = smf.ols(\"gave ~ treatment\", data=df).fit()\n# print(model_lin.summary())\n\nRegression Output Summary:\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nt-value\np-value\n95% CI\n\n\n\n\nIntercept\n0.0179\n0.001\n16.225\n0.000\n[0.016, 0.020]\n\n\nTreatment\n0.0042\n0.001\n3.101\n0.002\n[0.002, 0.007]\n\n\n\nThe t-test reveals a statistically significant difference in donation rates: the treatment group is more likely to give than the control group. This finding is confirmed by the regression, where the treatment coefficient is positive and significant, indicating that receiving a letter with a matching offer caused an increase in donations.\nThese results align closely with Table 2A, Panel A from Karlan & List (2007), which shows a response rate of 1.8% for the control group and 2.2% for the treatment group. The increase may seem small in absolute terms, but it is meaningful given the scale of the experiment and is statistically distinguishable from zero.\nSo, what does this tell us about human behavior? People are more likely to act generously when they feel their contribution is amplified. The idea that a matching donor will “double” or “triple” their impact likely makes the donation feel more meaningful. Even if the match ratio is relatively small, this added sense of social proof, urgency, or amplified impact nudges people to take action. In short, while most people didn’t give, those who were offered a matching donation were noticeably more likely to say yes. This demonstrates how small design changes in fundraising campaigns can have meaningful effects on human behavior.\nTo further analyze the effect of being offered a matching donation on the likelihood of giving, I estimate a probit regression, where the outcome variable is whether a charitable donation is made (gave), and the explanatory variable is treatment assignment.\n\n# Probit regression\nmodel_probit = smf.probit(\"gave ~ treatment\", data=df).fit()\n#print(model_probit.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\nRegression Output Summary:\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nz-value\np-value\n95% CI\n\n\n\n\nIntercept\n−2.1001\n0.023\n−90.073\n0.000\n[−2.146, −2.054]\n\n\nTreatment\n0.0868\n0.028\n3.113\n0.002\n[0.032, 0.141]\n\n\n\nThis result is statistically significant at the 1% level, and the sign and magnitude match closely with Table 3, Column 1 in the paper, where the authors report a coefficient of 0.086 with a standard error of 0.028.\nAlthough probit coefficients aren’t directly interpretable in terms of probability changes, the positive and significant result indicates that being in the treatment group increases the probability of donating.\nThis confirms what we see in the t-test and linear regression: offering a match increases donations, even when using a model tailored for binary outcomes.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nTo investigate whether larger match ratios (e.g., 2:1 or 3:1) encourage more people to donate than the 1:1 match, I run a series of t-tests comparing donation rates among the different match groups (all within the treatment group).\n\n\n\n\n\n\nPython Code to Compute Different t-tests\n\n\n\n\n\n\n# Subset to treatment group\ntreat_group = df[df[\"treatment\"] == 1]\n\n# Example: compare 2:1 vs 1:1\nx2 = treat_group[treat_group[\"ratio2\"] == 1][\"gave\"]\nx1 = treat_group[treat_group[\"ratio\"] == 1][\"gave\"]\n\nx_t, x_c = x2.mean(), x1.mean()\ns2_t, s2_c = x2.var(), x1.var()\nn_t, n_c = len(x2), len(x1)\n\nt_stat = (x_t - x_c) / ((s2_t/n_t + s2_c/n_c) ** 0.5)\n\n# Welch–Satterthwaite degrees of freedom\ndf_num = (s2_t/n_t + s2_c/n_c)**2\ndf_denom = ((s2_t/n_t)**2)/(n_t - 1) + ((s2_c/n_c)**2)/(n_c - 1)\ndof = df_num / df_denom\n\n# Two-sided p-value\np_val = 2 * t_dist.sf(abs(t_stat), dof)\n\n\n\n\n\nprint(f\"Manual t-stat: {t_stat:.3f}\")\nprint(f\"Manual p-value: {p_val:.4f}\")\n\nManual t-stat: 0.965\nManual p-value: 0.3345\n\n\nThe results are clear: there is no statistically significant differences in response rates between the 1:1, 2:1, and 3:1 match ratios. All t-statistics are approximately 0.965, and all p-values are around 0.335, well above conventional significance thresholds.\nThese results replicate the authors’ conclusion in the paper (see page 8), where they write: “…we do not find systematic patterns for the interaction effects.”\nIn other words, while offering a match increases donation rates (compared to no match), increasing the match ratio beyond 1:1 does not meaningfully boost donations further.\nTo complement the t-tests, I also run a regression on the subsample of individuals who received a matching donation. The goal is to estimate how donation likelihood varies across different match ratios: 1:1 (baseline), 2:1, and 3:1. I use an OLS regression where the dependent variable is gave, and the explanatory variables are indicator variables for the match ratios (ratio2, ratio3). The 1:1 match group serves as the reference category.\n\n# Run regression with 1:1 as baseline (ratio1 is omitted)\nmodel = smf.ols(\"gave ~ ratio2 + ratio3\", data=treat_group).fit()\n#print(model.summary())\n\nRegression Output Summary:\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nt-value\np-value\n95% CI\n\n\n\n\nIntercept\n0.0207\n0.001\n14.912\n0.000\n[0.018, 0.023]\n\n\nratio2 (2:1)\n0.0019\n0.002\n0.958\n0.338\n[−0.002, 0.006]\n\n\nratio3 (3:1)\n0.0020\n0.002\n1.008\n0.313\n[−0.002, 0.006]\n\n\n\nThis regression confirms the earlier t-test results: neither a 2:1 nor a 3:1 match rate leeds to a statistically significant increase in giving compared to a 1:1 match. The coefficients are small (~0.002) and not statistically significant (p &gt; 0.3). This suggests that offering a match matters more than the size of the match. Larger match offers (e.g., 3:1) do not appear to yield meaningful gains over a simple 1:1 offer.\nTo further assess whether larger match ratios led to more donations, I calculate the difference in donation rates between:\n\n2:1 vs 1:1\n3:1 vs 2:1\n3:1 vs 1:1\n\nFrom the treatment group, I compute the mean of gave for each match ratio group:\n\n\n\n\n\n\nRaw response rates by match ratio\n\n\n\n\n\n\n# Create match ratio group labels\ndef get_ratio(row):\n    if row[\"ratio2\"] == 1:\n        return \"2:1\"\n    elif row[\"ratio3\"] == 1:\n        return \"3:1\"\n    else:\n        return \"1:1\"\n\n# Safely add the new column\ntreat_group.loc[:, \"ratio_group\"] = treat_group.apply(get_ratio, axis=1)\n\n# Calculate mean donation rate by group\ndonation_rates = treat_group.groupby(\"ratio_group\")[\"gave\"].mean()\n\n/tmp/ipykernel_2892/2459960996.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\n\nprint(\"Raw Donation Rates by Match Ratio:\\n\", donation_rates)\n\nRaw Donation Rates by Match Ratio:\n ratio_group\n1:1    0.020749\n2:1    0.022633\n3:1    0.022733\nName: gave, dtype: float64\n\n\nSo:\n\n2:1 − 1:1 = 0.19 percentage points\n3:1 − 1:1 = 0.20 percentage points\n3:1 − 2:1 = 0.01 percentage points\n\nThese differences are very small.\nAlso, from the regression output, the coefficients for ratio2 and ratio3 are 0.0019 and 0.0020, respectively. These values again show that the 2:1 match group donated ~0.19 percentage points more than the 1:1 group; the 3:1 match group donated ~0.20 points more than the 1:1 group; and the difference between 3:1 and 2:1 is basically zero (0.0020 − 0.0019 = 0.0001).\nBoth the raw means and the regression results tell the same story: increasing the match ratio from 1:1 to 2:1 or 3:1 leads to slightly higher donation rates, but the increases are small and not statistically significant.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nFirst, to analyze whether the treatment (being offered a matching donation) influenced not just whether someone gave, but how much they gave, I conduct a t-test and a bivariate linear regression on the full sample (including individuals who gave $0).\n\n\n\n\n\n\nCompare donation amounts across all individuals\n\n\n\n\n\n\n# (most people donated $0)\nfrom scipy.stats import ttest_ind\ntreated = df[df[\"treatment\"] == 1][\"amount\"].dropna()\ncontrol = df[df[\"treatment\"] == 0][\"amount\"].dropna()\n\n# T-test\nt_stat, p_val = ttest_ind(treated, control, equal_var=False)\n\n# Linear regression (everyone)\nmodel_all = smf.ols(\"amount ~ treatment\", data=df).fit()\n\n\n\n\n\nprint(f\"T-test: t = {t_stat:.3f}, p = {p_val:.4f}\")\n#print(model_all.summary())\n\nT-test: t = 1.918, p = 0.0551\n\n\nRegression Output Summary:\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nt-value\np-value\n95% CI\n\n\n\n\nIntercept\n0.8133\n0.067\n12.063\n0.000\n[0.681, 0.945]\n\n\nTreatment\n0.1536\n0.083\n1.861\n0.063\n[−0.008, 0.315]\n\n\n\nThe t-test returns a t-statistic of 1.918 with a p-value of 0.0551, and the linear regression produces a treatment coefficient of 0.1536 (p = 0.063). These values are not statistically significant at the 5% level, but they are marginally close.\nThis analysis suggests that on average, individuals in the treatment group gave slightly more than those in the control group. However, this difference is not strong enough to conclude that the treatment definitively increased average donation size.\nImportantly, since the majority of individuals donated $0, this result primarily reflects differences in the likelihood of giving, not in the size of donations among actual donors. In other words, what we’re seeing is likely driven by extensive margin behavior — more people gave — rather than people giving more.\nTo truly assess whether the treatment affected the size of donations, we need to focus on those who actually gave, which is what we’ll do next.\nTo isolate the effect of the treatment on the amount donated, I restricted the analysis to only those individuals who actually gave. This lets us explore whether the matching offer not only encouraged more people to give, but also led them to give more money once they decided to donate.\nI ran a t-test and a linear regression of amount on treatment, using this restricted sample of donors.\n\n\n\n\n\n\nCompare donation amounts filtering only donors\n\n\n\n\n\n\n# Filter only donors (those who gave a positive amount)\ndonors = df[df[\"gave\"] == 1]\n\n# Compare mean donation amounts between treatment & control among donors\ntreated_donors = donors[donors[\"treatment\"] == 1][\"amount\"]\ncontrol_donors = donors[donors[\"treatment\"] == 0][\"amount\"]\n\nt_stat, p_val = ttest_ind(treated_donors, control_donors, equal_var=False)\n\n# Run regression: amount ~ treatment\nmodel_donors = smf.ols(\"amount ~ treatment\", data=donors).fit()\n\n\n\n\n\nprint(f\"T-test (donors only): t = {t_stat:.3f}, p = {p_val:.4f}\")\n#print(model_donors.summary())\n\nT-test (donors only): t = -0.585, p = 0.5590\n\n\nRegression Output Summary:\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nt-value\np-value\n95% CI\n\n\n\n\nIntercept\n45.5403\n2.423\n18.792\n0.000\n[40.785, 50.296]\n\n\nTreatment\n−1.6684\n2.872\n−0.581\n0.561\n[−7.305, 3.968]\n\n\n\nThese results indicate that, among donors, those who received the match offer actually gave slightly less than those in the control group, though the difference is not statistically significant.\nThis analysis suggests that the match offer was effective at getting more people to donate, but it did not increase the size of donations among those who were already willing to give. If anything, the point estimate implies the opposite, but the evidence is not strong enough to draw a firm conclusion.\nIn other words, the treatment worked on the extensive margin (getting more people to give), but not on the intensive margin (getting donors to give more).\nThe treatment coefficient doesn’t exactly have a causal interpretation, because, while the original experiment involved random assignment, this analysis conditions on a post-treatment variable (gave == 1), which may itself have been influenced by the treatment. This introduces selection bias, because the people who chose to give in each group may differ systematically.\nAs a result, the treatment coefficient here is descriptive (it tells us how much donors gave in each group) but it does not provide a valid causal estimate of how treatment affected donation size.\nFinally, to visualize how donation amounts differed between groups, I created two histograms (one for the treatment group and one for the control group) including only individuals who actually made a donation. A red vertical dashed line in each plot represents the average donation amount for that group.\n\nimport matplotlib.pyplot as plt\n\n# Subset only those who donated\ndonors = df[df[\"gave\"] == 1]\n\n# Split by treatment\ntreatment_donors = donors[donors[\"treatment\"] == 1][\"amount\"]\ncontrol_donors = donors[donors[\"treatment\"] == 0][\"amount\"]\n\n# Compute means\nmean_treat = treatment_donors.mean()\nmean_control = control_donors.mean()\n\n# Plot histograms\nfig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n\n# Control group plot\naxes[0].hist(control_donors, bins=30, color='skyblue', edgecolor='black')\naxes[0].axvline(mean_control, color='red', linestyle='dashed', linewidth=2, label=f\"Mean = {mean_control:.2f}\")\naxes[0].set_title(\"Control Group\")\naxes[0].set_xlabel(\"Donation Amount\")\naxes[0].set_ylabel(\"Frequency\")\naxes[0].legend()\n\n# Treatment group plot\naxes[1].hist(treatment_donors, bins=30, color='lightgreen', edgecolor='black')\naxes[1].axvline(mean_treat, color='red', linestyle='dashed', linewidth=2, label=f\"Mean = {mean_treat:.2f}\")\naxes[1].set_title(\"Treatment Group\")\naxes[1].set_xlabel(\"Donation Amount\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nObservations from the Plot\n\nThe distribution shapes are fairly similar across both groups: most donations fall below $100, with a few outliers in the $200–$400 range.\nThe average donation in the control group was $45.54, while the average in the treatment group was $43.87.\nDespite offering a matching donation, the treatment group did not donate more on average. In fact, they gave slightly less (though not significantly).\n\nThese findings match what we saw in the regression and t-test: while the treatment increased the likelihood of giving, it did not lead to larger donations among those who gave."
  },
  {
    "objectID": "blog/project1/index.html#simulation-experiment",
    "href": "blog/project1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nTo illustrate the Law of Large Numbers, I simulate donation behavior for two groups:\n\nControl group: Bernoulli trials with probability p=0.018 (1.8% chance of donating)\nTreatment group: Bernoulli trials with probability p=0.022 (2.2% chance of donating)\n\nAt each iteration, I simulate one draw from each distribution and record the difference (treatment − control). I repeat this 10,000 times, then plotted the cumulative average of these differences across iterations. The red dashed line in the plot shows the true difference in donation probabilities: True difference=0.022−0.018=0.004\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Simulation parameters\nn = 10000\np_control = 0.018\np_treatment = 0.022\n\n# Simulate donations (1 = gave, 0 = didn't)\ncontrol_draws = np.random.binomial(1, p_control, n)\ntreatment_draws = np.random.binomial(1, p_treatment, n)\n\n# Compute stepwise differences in donation outcomes\ndiffs = treatment_draws - control_draws\n\n# Cumulative average of differences\ncumulative_avg_diff = np.cumsum(diffs) / np.arange(1, n+1)\n\n# Plot\nplt.figure(figsize=(10, 4))\nplt.plot(cumulative_avg_diff, label='Cumulative Average Difference', color='blue')\nplt.axhline(p_treatment - p_control, color='red', linestyle='dashed', label=f\"True Diff = {p_treatment - p_control:.004f}\")\nplt.title(\"Law of Large Numbers: Cumulative Average Difference\")\nplt.xlabel(\"Simulation Iteration\")\nplt.ylabel(\"Average Difference in Donation Rates\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nIn this plot we can see taht, initially, the cumulative average fluctuates heavily (the early estimates are noisy due to small sample sizes). But as the number of simulated draws increases, the average converges toward the true value, stabilizing close to 0.004.\nThis is a classic visual demonstration of the Law of Large Numbers, which tells us that, as the number of observations grows, the sample mean converges to the population mean.\n\n\nCentral Limit Theorem\nTo visualize the Central Limit Theorem, I simulate the average difference in donation rates between treatment and control groups using four different sample sizes: 50, 200, 500, and 1000. For each sample size, I draw samples of size n from Bernoulli distributions with a probabilities p=0.018 and p=0.022, for the control group and treatment group respectively. Then, I compute the difference in sample means between the groups, and I repeat this process 1000 times, generating 1000 average differences for each sample size. I then plot a histogram of those 1000 differences.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed\nnp.random.seed(42)\n\n# Parameters\np_control = 0.018\np_treatment = 0.022\nsample_sizes = [50, 200, 500, 1000]\nnum_simulations = 1000\n\n# Create subplots\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    avg_diffs = []\n\n    for _ in range(num_simulations):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        avg_diffs.append(diff)\n\n    # Plot histogram\n    axes[i].hist(avg_diffs, bins=30, color='lightblue', edgecolor='black')\n    axes[i].axvline(x=0, color='red', linestyle='dashed', label='Zero')\n    axes[i].axvline(x=np.mean(avg_diffs), color='black', linestyle='dotted', label='Sample Mean')\n    axes[i].set_title(f\"Sample Size = {n}\")\n    axes[i].set_xlabel(\"Avg. Difference in Donation Rates\")\n    axes[i].set_ylabel(\"Frequency\")\n    axes[i].legend()\n\nplt.suptitle(\"Central Limit Theorem: Distribution of Average Differences\", fontsize=14)\nplt.tight_layout()\nplt.subplots_adjust(top=0.9)\nplt.show()\n\n\n\n\n\n\n\n\n\nWhat the Plots Show\n\nSample Size = 50: The distribution is wide and somewhat irregular. It includes zero near the center, suggesting that with small samples, random noise can easily obscure the true effect.\nSample Size = 200: The distribution becomes narrower and more symmetric. The average difference starts to center around the true difference (~0.004).\nSample Size = 500: The shape becomes more bell-like, and zero begins to move toward the edge of the distribution.\nSample Size = 1000: The distribution is clearly approximately normal and tightly concentrated around the true average difference. Zero is now well into the tail, indicating that it’s increasingly unlikely the observed difference is due to chance.\n\nThis simulation confirms the Central Limit Theorem in practice: As sample size increases, the distribution of the sample mean becomes approximately normal, regardless of the underlying distribution, and its standard error decreases.\nIn the context of the donation experiment, it shows that with small samples, detecting a small effect like a 0.4 percentage point increase is hard and noisy. But as the sample size grows, the signal becomes more visible, and zero becomes a less likely explanation for the difference.\nThis reinforces the value of large samples in experiments, since they provide more precise, reliable estimates of treatment effects."
  },
  {
    "objectID": "HW2/HW1_data.html",
    "href": "HW2/HW1_data.html",
    "title": "Data",
    "section": "",
    "text": "import pandas as pd\n\nblueprinty = pd.read_csv('blueprinty.csv')\nblueprinty.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Compute mean number of patents by customer status\nmean_patents = blueprinty.groupby(\"iscustomer\")[\"patents\"].mean()\n\n# Plot histogram of number of patents by customer status\nplt.figure(figsize=(10, 5))\nsns.histplot(data=blueprinty, x=\"patents\", hue=\"iscustomer\", multiple=\"dodge\", bins=20)\nplt.title(\"Histogram of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Is Customer\", labels=[\"Non-Customer\", \"Customer\"])\nplt.tight_layout()\n\nmean_patents\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n# Boxplot for firm age by customer status\nplt.figure(figsize=(8, 5))\nsns.boxplot(x=\"iscustomer\", y=\"age\", data=blueprinty)\nplt.title(\"Firm Age by Customer Status\")\nplt.xlabel(\"Is Customer\")\nplt.ylabel(\"Firm Age\")\nplt.xticks([0, 1], [\"Non-Customer\", \"Customer\"])\nplt.tight_layout()\n\n# Crosstab for region by customer status\nregion_customer_crosstab = pd.crosstab(blueprinty[\"region\"], blueprinty[\"iscustomer\"], normalize='index') * 100\nregion_customer_crosstab.round(1)\n\n\n\n\n\n\n\niscustomer\n0\n1\n\n\nregion\n\n\n\n\n\n\nMidwest\n83.5\n16.5\n\n\nNortheast\n45.4\n54.6\n\n\nNorthwest\n84.5\n15.5\n\n\nSouth\n81.7\n18.3\n\n\nSouthwest\n82.5\n17.5"
  },
  {
    "objectID": "HW2/HW1_data.html#estimation-of-simple-poisson-model",
    "href": "HW2/HW1_data.html#estimation-of-simple-poisson-model",
    "title": "Data",
    "section": "Estimation of Simple Poisson Model",
    "text": "Estimation of Simple Poisson Model\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_loglikelihood(lmbda, Y):\n    \"\"\"\n    Computes the negative log-likelihood of a Poisson model with scalar lambda.\n    This is suitable for optimization routines like scipy.optimize.minimize.\n    \n    Parameters:\n        lmbda : float\n            The Poisson rate parameter (λ), must be &gt; 0.\n        Y : array-like\n            Observed count data (e.g., number of patents).\n\n    Returns:\n        float : negative log-likelihood\n    \"\"\"\n    if lmbda &lt;= 0:\n        return np.inf  # avoid invalid log(λ) or exp(−λ)\n    \n    Y = np.asarray(Y)\n    log_lik = np.sum(Y * np.log(lmbda) - lmbda - gammaln(Y + 1))\n    return log_lik  \n\n\nimport matplotlib.pyplot as plt\n\nY = blueprinty[\"patents\"].values\n\nlambda_vals = np.linspace(0.1, 10, 200)\nloglik_vals = [poisson_loglikelihood(lmb, Y) for lmb in lambda_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, color='darkblue')\nplt.title(\"Poisson Log-Likelihood vs. Lambda\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy.optimize import minimize\n\n# Define the negative log-likelihood for use with optimizer\ndef neg_poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return np.inf\n    return -poisson_loglikelihood(lmbda, Y)\n\n# Use the same Y from earlier\nY = blueprinty[\"patents\"].values\n\n# Use scipy to minimize the negative log-likelihood\nresult = minimize(fun=neg_poisson_loglikelihood, x0=[1.0], args=(Y,), bounds=[(1e-6, None)])\nlambda_mle = result.x[0]\nlambda_mle\n\n3.6846667021660804"
  },
  {
    "objectID": "HW2/HW1_data.html#estimation-of-poisson-regression-model",
    "href": "HW2/HW1_data.html#estimation-of-poisson-regression-model",
    "title": "Data",
    "section": "Estimation of Poisson Regression Model",
    "text": "Estimation of Poisson Regression Model\n\nimport numpy as np\nfrom scipy.special import gammaln\n\n# Define Poisson log-likelihood function (negative, stabilized)\ndef poisson_regression_loglik(beta, Y, X):\n    eta = X @ beta\n    eta = np.clip(eta, -20, 20)        # Prevent overflow in exp()\n    lam = np.exp(eta)\n    log_lik = np.sum(Y * eta - lam - gammaln(Y + 1))\n    return -log_lik                   # Return negative for minimizer\n\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\nimport patsy\n\n# Create design matrix with intercept, age, age², region dummies, and customer\ny, X = patsy.dmatrices('patents ~ age + I(age**2) + C(region) + iscustomer', data=blueprinty, return_type='dataframe')\nY = y.values.flatten()  # Convert y to 1D\n\n# Initial guess for beta\nbeta0 = np.zeros(X.shape[1])\n\n# Optimize the likelihood\nresult = minimize(poisson_regression_loglik, beta0, args=(Y, X), method='BFGS')\n\n# Extract MLE estimates and standard errors\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n# Present results in table\nresults_df = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": standard_errors\n}, index=X.columns)\n\nprint(results_df.round(4))\n\n                        Coefficient  Std. Error\nIntercept                   -0.5100      0.1931\nC(region)[T.Northeast]       0.0292      0.0468\nC(region)[T.Northwest]      -0.0176      0.0572\nC(region)[T.South]           0.0566      0.0562\nC(region)[T.Southwest]       0.0506      0.0497\nage                          0.1487      0.0145\nI(age ** 2)                 -0.0030      0.0003\niscustomer                   0.2076      0.0329\n\n\n\nimport statsmodels.api as sm\n\n# Fit Poisson GLM with log link\nglm_model = sm.GLM(Y, X, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Display summary table\nprint(glm_results.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Sun, 04 May 2025   Deviance:                       2143.3\nTime:                        17:40:46   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n==========================================================================================\n                             coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------------------\nIntercept                 -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nC(region)[T.Northeast]     0.0292      0.044      0.669      0.504      -0.056       0.115\nC(region)[T.Northwest]    -0.0176      0.054     -0.327      0.744      -0.123       0.088\nC(region)[T.South]         0.0566      0.053      1.074      0.283      -0.047       0.160\nC(region)[T.Southwest]     0.0506      0.047      1.072      0.284      -0.042       0.143\nage                        0.1486      0.014     10.716      0.000       0.121       0.176\nI(age ** 2)               -0.0030      0.000    -11.513      0.000      -0.003      -0.002\niscustomer                 0.2076      0.031      6.719      0.000       0.147       0.268\n==========================================================================================\n\n\n\n# Create two counterfactual design matrices:\n# - X_0: assumes all firms are non-customers (iscustomer = 0)\n# - X_1: assumes all firms are customers (iscustomer = 1)\nX_0 = X.copy()\nX_1 = X.copy()\n\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n# Predict expected number of patents under both scenarios\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\n# Compute difference and average effect\ndiff = y_pred_1 - y_pred_0\naverage_effect = diff.mean()\n\nprint(f\"Average treatment effect of Blueprinty's software: {average_effect:.4f} additional patents\")\n\nAverage treatment effect of Blueprinty's software: 0.7928 additional patents"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blog/project2/index.html",
    "href": "blog/project2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nWe compare the number of patents held by firms that are Blueprinty customers vs. non-customers.\n\nimport pandas as pd\n\nblueprinty = pd.read_csv('blueprinty.csv')\nblueprinty.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nThe following histogram shows that Blueprinty customers (orange bars) tend to have slightly more patents than non-customers. The distribution is right-skewed for both groups, but the customer group has more firms with 5+ patents. Additionally, the mean number of patents for non-customers is 3.47 and for customers is 4.13.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Compute mean number of patents by customer status\nmean_patents = blueprinty.groupby(\"iscustomer\")[\"patents\"].mean()\n\n# Plot histogram of number of patents by customer status\nplt.figure(figsize=(10, 5))\nsns.histplot(data=blueprinty, x=\"patents\", hue=\"iscustomer\", multiple=\"dodge\", bins=20)\nplt.title(\"Histogram of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Is Customer\", labels=[\"Non-Customer\", \"Customer\"])\nplt.tight_layout()\n\nmean_patents\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\n\n\n\n\n\n\nObservation: There is a moderate difference in average patent count between customers and non-customers. This suggests that Blueprinty customers may be more successful in generating patents — though this raw difference may reflect underlying differences (e.g. age, region), not necessarily a causal effect of using the software.\nFor this reason, since Blueprinty customers are not selected at random, it may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Boxplot for firm age by customer status\nplt.figure(figsize=(8, 5))\nsns.boxplot(x=\"iscustomer\", y=\"age\", data=blueprinty)\nplt.title(\"Firm Age by Customer Status\")\nplt.xlabel(\"Is Customer\")\nplt.ylabel(\"Firm Age\")\nplt.xticks([0, 1], [\"Non-Customer\", \"Customer\"])\nplt.tight_layout()\n\n# Crosstab for region by customer status\nregion_customer_crosstab = pd.crosstab(blueprinty[\"region\"], blueprinty[\"iscustomer\"], normalize='index') * 100\nregion_customer_crosstab.round(1)\n\n\n\n\n\n\n\niscustomer\n0\n1\n\n\nregion\n\n\n\n\n\n\nMidwest\n83.5\n16.5\n\n\nNortheast\n45.4\n54.6\n\n\nNorthwest\n84.5\n15.5\n\n\nSouth\n81.7\n18.3\n\n\nSouthwest\n82.5\n17.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfter comparing regions and ages by customer status, we can see that, for the firm age, the boxplot shows that Blueprinty customers and non-customers have similar age distributions. However, customers have a slightly higher median age, and both groups show wide variation, with some firms as old as ~50 years. The overall distributions are fairly overlapping.\nOn the other hand, there are large differences in the regional distribution. Northeast has the highest customer share (54.6% of firms in this region are customers). In contrast, in regions like Midwest, Northwest, South, and Southwest, only ~15–18% of firms are customers.\nObservations: There is non-random customer selection: - Age is fairly balanced but might still matter. - Region is clearly associated with customer status, and firms in the Northeast are far more likely to be Blueprinty customers.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWe assume that each observation \\(Y_i\\) follows a Poisson distribution:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda)\n\\]\nThis means the probability mass function is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nAssuming the \\(Y_i\\) are i.i.d., the likelihood function is:\n\\[\n\\mathcal{L}(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{y_i}}{y_i!}\n\\]\nTaking the natural logarithm of the likelihood, we obtain the log-likelihood function:\n\\[\n\\ell(\\lambda) = \\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^n \\left[ y_i \\log(\\lambda) - \\lambda - \\log(y_i!) \\right]\n\\]\nNext, we define the log-likelihood for the Poisson distribution as a function of a single parameter, \\(\\lambda\\), and the observed data, \\(Y\\). This function is maximized to estimate the most likely value of \\(\\lambda\\) that could have generated the data:\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return np.inf  # avoid invalid log(λ) or exp(−λ)\n    Y = np.asarray(Y)\n    log_lik = np.sum(Y * np.log(lmbda) - lmbda - gammaln(Y + 1))\n    return log_lik  # negative for use with minimization\n\nWe then evaluate this function over a range of candidate values for \\(\\lambda\\), using the observed number of patents in the dataset. This helps us visually identify the value of \\(\\lambda\\) that maximizes the log-likelihood:\n\nY = blueprinty[\"patents\"].values\n\nlambda_vals = np.linspace(0.1, 10, 200)\nloglik_vals = [poisson_loglikelihood(lmb, Y) for lmb in lambda_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, color='darkblue')\nplt.title(\"Poisson Log-Likelihood vs. Lambda\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis plot reveals a clear peak in the log-likelihood function, indicating the most likely value of \\(\\lambda\\) given the observed number of patents. The shape confirms that the log-likelihood is concave, making optimization straightforward.\nTo confirm our visual result mathematically, we take the derivative of the log-likelihood function and solve for the value of \\(\\lambda\\) that maximizes it.\nStarting from the log-likelihood:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left[ y_i \\log(\\lambda) - \\lambda - \\log(y_i!) \\right]\n\\]\nTake the derivative with respect to \\(\\lambda\\):\n\\[\n\\frac{d\\ell}{d\\lambda} = \\frac{1}{\\lambda} \\sum_{i=1}^n y_i - n\n\\]\nSet the derivative to zero and solve:\n\\[\n\\begin{gather*}\n\\frac{1}{\\lambda} \\sum_{i=1}^n y_i - n = 0 \\\\\n\\sum_{i=1}^n y_i = n \\lambda \\\\\n\\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n y_i = \\bar{y}\n\\end{gather*}\n\\]\nConclusion: The MLE for \\(\\lambda\\) is simply the sample mean of the observed counts (\\(\\hat{\\lambda}_{\\text{MLE}} = \\bar{y}\\)). This aligns with our numerical result and reinforces the interpretation of \\(\\lambda\\) as the expected number of events (in this case, patents per firm).\nFinally, using scipy.optimize.minimize(), we numerically maximized the Poisson log-likelihood and obtained:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = 3.685\n\\]\nThis matches the sample mean of the observed patent counts, which aligns with our analytical derivation. The optimizer successfully found the value of \\(\\lambda\\) that maximizes the likelihood of observing the data under a Poisson model.\n\nfrom scipy.optimize import minimize\n\n# Define the negative log-likelihood for use with optimizer\ndef neg_poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return np.inf\n    return -poisson_loglikelihood(lmbda, Y)\n\n# Use the same Y from earlier\nY = blueprinty[\"patents\"].values\n\n# Use scipy to minimize the negative log-likelihood\nresult = minimize(fun=neg_poisson_loglikelihood, x0=[1.0], args=(Y,), bounds=[(1e-6, None)])\nlambda_mle = result.x[0]\nlambda_mle\n\n3.6846667021660804\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe now extend our Poisson likelihood to a regression setting, where the rate parameter \\(\\lambda_i\\) varies across observations. Specifically, we assume:\n\\[\n\\lambda_i = \\exp(X_i' \\beta)\n\\]\nThis ensures \\(\\lambda_i &gt; 0\\), as required by the Poisson distribution. The log-likelihood function becomes:\n\\[\n\\ell(\\beta) = \\sum_{i=1}^n \\left[ y_i (X_i' \\beta) - \\exp(X_i' \\beta) - \\log(y_i!) \\right]\n\\]\nWe implement this in Python by defining a function that takes the coefficient vector \\(\\beta\\), the observed count data \\(Y\\), and the covariate matrix \\(X\\):\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_loglik(beta, Y, X):\n    eta = X @ beta\n    eta = np.clip(eta, -20, 20)        # Prevent overflow in exp()\n    lam = np.exp(eta)\n    log_lik = np.sum(Y * eta - lam - gammaln(Y + 1))\n    return -log_lik                   # Return negative for minimizer\n\nTo estimate the Poisson regression model, we use Maximum Likelihood Estimation (MLE) via Python’s scipy.optimize.minimize() function. The goal is to obtain the MLE vector \\(\\hat{\\beta}\\) and compute the standard errors using the inverse of the Hessian matrix. We construct the covariate matrix \\(X\\) as follows:\n\nThe first column is a constant (1’s), enabling an intercept in the model.\nNext are: age, age², dummy variables for all but one region (reference category), and a binary indicator for whether the firm is a Blueprinty customer.\n\n\nimport patsy\n# Create design matrix with intercept, age, age², region dummies, and customer\ny, X = patsy.dmatrices('patents ~ age + I(age**2) + C(region) + iscustomer', data=blueprinty, return_type='dataframe')\nY = y.values.flatten()  # Convert y to 1D\n\n# Initial guess for beta\nbeta0 = np.zeros(X.shape[1])\n\n# Optimize the likelihood\nresult = minimize(poisson_regression_loglik, beta0, args=(Y, X), method='BFGS')\n\n# Extract MLE estimates and standard errors\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n# Present results in table\nresults_df = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": standard_errors\n}, index=X.columns)\n\nresults_df.round(4)\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nIntercept\n-0.5100\n0.1931\n\n\nC(region)[T.Northeast]\n0.0292\n0.0468\n\n\nC(region)[T.Northwest]\n-0.0176\n0.0572\n\n\nC(region)[T.South]\n0.0566\n0.0562\n\n\nC(region)[T.Southwest]\n0.0506\n0.0497\n\n\nage\n0.1487\n0.0145\n\n\nI(age ** 2)\n-0.0030\n0.0003\n\n\niscustomer\n0.2076\n0.0329\n\n\n\n\n\n\n\nBy including the intercept and relevant covariates in our design matrix and maximizing the log-likelihood using numerical optimization, we obtain statistically valid parameter estimates. These results allow us to rigorously assess the effect of Blueprinty’s software (and other variables) on the number of patents.\nTo verify the correctness of our manually optimized Poisson regression, we re-estimate the model using Python’s built-in Generalized Linear Model (GLM) implementation from the statsmodels package.\nThis model also uses the Poisson family with a log link, which matches the specification of our manual log-likelihood.\n\n\n\n\n\n\nResults using Python sm.GLM() function\n\n\n\n\n\n\nimport statsmodels.api as sm\n\n# Fit Poisson GLM with log link\nglm_model = sm.GLM(Y, X, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Display summary table\nprint(glm_results.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Mon, 05 May 2025   Deviance:                       2143.3\nTime:                        13:12:36   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n==========================================================================================\n                             coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------------------\nIntercept                 -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nC(region)[T.Northeast]     0.0292      0.044      0.669      0.504      -0.056       0.115\nC(region)[T.Northwest]    -0.0176      0.054     -0.327      0.744      -0.123       0.088\nC(region)[T.South]         0.0566      0.053      1.074      0.283      -0.047       0.160\nC(region)[T.Southwest]     0.0506      0.047      1.072      0.284      -0.042       0.143\nage                        0.1486      0.014     10.716      0.000       0.121       0.176\nI(age ** 2)               -0.0030      0.000    -11.513      0.000      -0.003      -0.002\niscustomer                 0.2076      0.031      6.719      0.000       0.147       0.268\n==========================================================================================\n\n\n\n\n\nInterpreting the Poisson Regression Results:\nFrom the comparison between our custom MLE output and statsmodels.GLM, we observe nearly identical coefficient estimates and standard errors, confirming that our implementation is correct.\nThe most notable results include:\n\niscustomer:\n\nCoefficient: 0.2076, Standard Error: 0.0329\nHighly statistically significant (p&lt;0.001)\nInterpretation: Firms using Blueprinty’s software are associated with a 23% increase in expected patent counts, all else equal. This is computed by exponentiating the coefficient (\\(\\exp (0.2076) \\approx 1.23\\))\n\nage:\n\nCoefficient: 0.1487, highly significant.\nSuggests that older firms tend to generate more patents.\n\nage ** 2:\n\nCoefficient: −0.0030, also highly significant.\nIndicates a diminishing return to age: very old firms may become less innovative or productive.\n\nRegion Dummie:\n\nNone of the region dummy variables are statistically significant at the 5% level.\nSuggests no strong regional effect on patent counts after controlling for other factors.\n\n\nTo summarize, the model provides strong evidence that Blueprinty’s software is associated with higher patent productivity. Also, the quadratic form of age shows that patent output rises with experience, but not indefinitely. Regional effects are minimal once other variables are included. These insights can help Blueprinty support its value proposition and target more impactful segments.\n\n\nBecause the coefficient from a Poisson regression model is not directly interpretable as a marginal effect, we estimate the effect of Blueprinty’s software by comparing counterfactual predictions.\nTo do so, we create two hypothetical versions of the dataset:\n\n\\(X_0\\): All firms set to iscustomer = 0 (non-users)\n\\(X_1\\): All firms set to iscustomer = 1 (Blueprinty users)\n\nUsing our fitted model, we compute the expected number of patents under both scenarios:\n\nX_0 = X.copy()\nX_1 = X.copy()\n\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n# Predict expected number of patents under both scenarios\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\n# Compute difference and average effect\ndiff = y_pred_1 - y_pred_0\naverage_effect = diff.mean()\n\nprint(f\"Average treatment effect of Blueprinty's software: {average_effect:.4f} additional patents\")\n\nAverage treatment effect of Blueprinty's software: 0.7928 additional patents\n\n\nThe estimated average treatment effect of Blueprinty’s software is 0.7928. This means that, on average, firms using Blueprinty’s software are predicted to obtain approximately 0.79 more patents over the 5-year period, holding all other firm characteristics constant.\n\n\n\nWe conclude that Blueprinty’s software is associated with a positive and meaningful increase in patent success:\n\nThe marginal effect of using the software is +0.79 patents on average.\nCombined with the earlier result that iscustomer was statistically significant in the regression, this provides strong evidence that Blueprinty’s product has a measurable and valuable effect on innovation outcomes."
  },
  {
    "objectID": "blog/project2/index.html#blueprinty-case-study",
    "href": "blog/project2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nWe compare the number of patents held by firms that are Blueprinty customers vs. non-customers.\n\nimport pandas as pd\n\nblueprinty = pd.read_csv('blueprinty.csv')\nblueprinty.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nThe following histogram shows that Blueprinty customers (orange bars) tend to have slightly more patents than non-customers. The distribution is right-skewed for both groups, but the customer group has more firms with 5+ patents. Additionally, the mean number of patents for non-customers is 3.47 and for customers is 4.13.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Compute mean number of patents by customer status\nmean_patents = blueprinty.groupby(\"iscustomer\")[\"patents\"].mean()\n\n# Plot histogram of number of patents by customer status\nplt.figure(figsize=(10, 5))\nsns.histplot(data=blueprinty, x=\"patents\", hue=\"iscustomer\", multiple=\"dodge\", bins=20)\nplt.title(\"Histogram of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Is Customer\", labels=[\"Non-Customer\", \"Customer\"])\nplt.tight_layout()\n\nmean_patents\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\n\n\n\n\n\n\nObservation: There is a moderate difference in average patent count between customers and non-customers. This suggests that Blueprinty customers may be more successful in generating patents — though this raw difference may reflect underlying differences (e.g. age, region), not necessarily a causal effect of using the software.\nFor this reason, since Blueprinty customers are not selected at random, it may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Boxplot for firm age by customer status\nplt.figure(figsize=(8, 5))\nsns.boxplot(x=\"iscustomer\", y=\"age\", data=blueprinty)\nplt.title(\"Firm Age by Customer Status\")\nplt.xlabel(\"Is Customer\")\nplt.ylabel(\"Firm Age\")\nplt.xticks([0, 1], [\"Non-Customer\", \"Customer\"])\nplt.tight_layout()\n\n# Crosstab for region by customer status\nregion_customer_crosstab = pd.crosstab(blueprinty[\"region\"], blueprinty[\"iscustomer\"], normalize='index') * 100\nregion_customer_crosstab.round(1)\n\n\n\n\n\n\n\niscustomer\n0\n1\n\n\nregion\n\n\n\n\n\n\nMidwest\n83.5\n16.5\n\n\nNortheast\n45.4\n54.6\n\n\nNorthwest\n84.5\n15.5\n\n\nSouth\n81.7\n18.3\n\n\nSouthwest\n82.5\n17.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfter comparing regions and ages by customer status, we can see that, for the firm age, the boxplot shows that Blueprinty customers and non-customers have similar age distributions. However, customers have a slightly higher median age, and both groups show wide variation, with some firms as old as ~50 years. The overall distributions are fairly overlapping.\nOn the other hand, there are large differences in the regional distribution. Northeast has the highest customer share (54.6% of firms in this region are customers). In contrast, in regions like Midwest, Northwest, South, and Southwest, only ~15–18% of firms are customers.\nObservations: There is non-random customer selection: - Age is fairly balanced but might still matter. - Region is clearly associated with customer status, and firms in the Northeast are far more likely to be Blueprinty customers.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWe assume that each observation \\(Y_i\\) follows a Poisson distribution:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda)\n\\]\nThis means the probability mass function is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nAssuming the \\(Y_i\\) are i.i.d., the likelihood function is:\n\\[\n\\mathcal{L}(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{y_i}}{y_i!}\n\\]\nTaking the natural logarithm of the likelihood, we obtain the log-likelihood function:\n\\[\n\\ell(\\lambda) = \\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^n \\left[ y_i \\log(\\lambda) - \\lambda - \\log(y_i!) \\right]\n\\]\nNext, we define the log-likelihood for the Poisson distribution as a function of a single parameter, \\(\\lambda\\), and the observed data, \\(Y\\). This function is maximized to estimate the most likely value of \\(\\lambda\\) that could have generated the data:\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return np.inf  # avoid invalid log(λ) or exp(−λ)\n    Y = np.asarray(Y)\n    log_lik = np.sum(Y * np.log(lmbda) - lmbda - gammaln(Y + 1))\n    return log_lik  # negative for use with minimization\n\nWe then evaluate this function over a range of candidate values for \\(\\lambda\\), using the observed number of patents in the dataset. This helps us visually identify the value of \\(\\lambda\\) that maximizes the log-likelihood:\n\nY = blueprinty[\"patents\"].values\n\nlambda_vals = np.linspace(0.1, 10, 200)\nloglik_vals = [poisson_loglikelihood(lmb, Y) for lmb in lambda_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, color='darkblue')\nplt.title(\"Poisson Log-Likelihood vs. Lambda\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis plot reveals a clear peak in the log-likelihood function, indicating the most likely value of \\(\\lambda\\) given the observed number of patents. The shape confirms that the log-likelihood is concave, making optimization straightforward.\nTo confirm our visual result mathematically, we take the derivative of the log-likelihood function and solve for the value of \\(\\lambda\\) that maximizes it.\nStarting from the log-likelihood:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left[ y_i \\log(\\lambda) - \\lambda - \\log(y_i!) \\right]\n\\]\nTake the derivative with respect to \\(\\lambda\\):\n\\[\n\\frac{d\\ell}{d\\lambda} = \\frac{1}{\\lambda} \\sum_{i=1}^n y_i - n\n\\]\nSet the derivative to zero and solve:\n\\[\n\\begin{gather*}\n\\frac{1}{\\lambda} \\sum_{i=1}^n y_i - n = 0 \\\\\n\\sum_{i=1}^n y_i = n \\lambda \\\\\n\\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n y_i = \\bar{y}\n\\end{gather*}\n\\]\nConclusion: The MLE for \\(\\lambda\\) is simply the sample mean of the observed counts (\\(\\hat{\\lambda}_{\\text{MLE}} = \\bar{y}\\)). This aligns with our numerical result and reinforces the interpretation of \\(\\lambda\\) as the expected number of events (in this case, patents per firm).\nFinally, using scipy.optimize.minimize(), we numerically maximized the Poisson log-likelihood and obtained:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = 3.685\n\\]\nThis matches the sample mean of the observed patent counts, which aligns with our analytical derivation. The optimizer successfully found the value of \\(\\lambda\\) that maximizes the likelihood of observing the data under a Poisson model.\n\nfrom scipy.optimize import minimize\n\n# Define the negative log-likelihood for use with optimizer\ndef neg_poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return np.inf\n    return -poisson_loglikelihood(lmbda, Y)\n\n# Use the same Y from earlier\nY = blueprinty[\"patents\"].values\n\n# Use scipy to minimize the negative log-likelihood\nresult = minimize(fun=neg_poisson_loglikelihood, x0=[1.0], args=(Y,), bounds=[(1e-6, None)])\nlambda_mle = result.x[0]\nlambda_mle\n\n3.6846667021660804\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe now extend our Poisson likelihood to a regression setting, where the rate parameter \\(\\lambda_i\\) varies across observations. Specifically, we assume:\n\\[\n\\lambda_i = \\exp(X_i' \\beta)\n\\]\nThis ensures \\(\\lambda_i &gt; 0\\), as required by the Poisson distribution. The log-likelihood function becomes:\n\\[\n\\ell(\\beta) = \\sum_{i=1}^n \\left[ y_i (X_i' \\beta) - \\exp(X_i' \\beta) - \\log(y_i!) \\right]\n\\]\nWe implement this in Python by defining a function that takes the coefficient vector \\(\\beta\\), the observed count data \\(Y\\), and the covariate matrix \\(X\\):\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_loglik(beta, Y, X):\n    eta = X @ beta\n    eta = np.clip(eta, -20, 20)        # Prevent overflow in exp()\n    lam = np.exp(eta)\n    log_lik = np.sum(Y * eta - lam - gammaln(Y + 1))\n    return -log_lik                   # Return negative for minimizer\n\nTo estimate the Poisson regression model, we use Maximum Likelihood Estimation (MLE) via Python’s scipy.optimize.minimize() function. The goal is to obtain the MLE vector \\(\\hat{\\beta}\\) and compute the standard errors using the inverse of the Hessian matrix. We construct the covariate matrix \\(X\\) as follows:\n\nThe first column is a constant (1’s), enabling an intercept in the model.\nNext are: age, age², dummy variables for all but one region (reference category), and a binary indicator for whether the firm is a Blueprinty customer.\n\n\nimport patsy\n# Create design matrix with intercept, age, age², region dummies, and customer\ny, X = patsy.dmatrices('patents ~ age + I(age**2) + C(region) + iscustomer', data=blueprinty, return_type='dataframe')\nY = y.values.flatten()  # Convert y to 1D\n\n# Initial guess for beta\nbeta0 = np.zeros(X.shape[1])\n\n# Optimize the likelihood\nresult = minimize(poisson_regression_loglik, beta0, args=(Y, X), method='BFGS')\n\n# Extract MLE estimates and standard errors\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n# Present results in table\nresults_df = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": standard_errors\n}, index=X.columns)\n\nresults_df.round(4)\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nIntercept\n-0.5100\n0.1931\n\n\nC(region)[T.Northeast]\n0.0292\n0.0468\n\n\nC(region)[T.Northwest]\n-0.0176\n0.0572\n\n\nC(region)[T.South]\n0.0566\n0.0562\n\n\nC(region)[T.Southwest]\n0.0506\n0.0497\n\n\nage\n0.1487\n0.0145\n\n\nI(age ** 2)\n-0.0030\n0.0003\n\n\niscustomer\n0.2076\n0.0329\n\n\n\n\n\n\n\nBy including the intercept and relevant covariates in our design matrix and maximizing the log-likelihood using numerical optimization, we obtain statistically valid parameter estimates. These results allow us to rigorously assess the effect of Blueprinty’s software (and other variables) on the number of patents.\nTo verify the correctness of our manually optimized Poisson regression, we re-estimate the model using Python’s built-in Generalized Linear Model (GLM) implementation from the statsmodels package.\nThis model also uses the Poisson family with a log link, which matches the specification of our manual log-likelihood.\n\n\n\n\n\n\nResults using Python sm.GLM() function\n\n\n\n\n\n\nimport statsmodels.api as sm\n\n# Fit Poisson GLM with log link\nglm_model = sm.GLM(Y, X, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Display summary table\nprint(glm_results.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Mon, 05 May 2025   Deviance:                       2143.3\nTime:                        13:12:36   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n==========================================================================================\n                             coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------------------\nIntercept                 -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nC(region)[T.Northeast]     0.0292      0.044      0.669      0.504      -0.056       0.115\nC(region)[T.Northwest]    -0.0176      0.054     -0.327      0.744      -0.123       0.088\nC(region)[T.South]         0.0566      0.053      1.074      0.283      -0.047       0.160\nC(region)[T.Southwest]     0.0506      0.047      1.072      0.284      -0.042       0.143\nage                        0.1486      0.014     10.716      0.000       0.121       0.176\nI(age ** 2)               -0.0030      0.000    -11.513      0.000      -0.003      -0.002\niscustomer                 0.2076      0.031      6.719      0.000       0.147       0.268\n==========================================================================================\n\n\n\n\n\nInterpreting the Poisson Regression Results:\nFrom the comparison between our custom MLE output and statsmodels.GLM, we observe nearly identical coefficient estimates and standard errors, confirming that our implementation is correct.\nThe most notable results include:\n\niscustomer:\n\nCoefficient: 0.2076, Standard Error: 0.0329\nHighly statistically significant (p&lt;0.001)\nInterpretation: Firms using Blueprinty’s software are associated with a 23% increase in expected patent counts, all else equal. This is computed by exponentiating the coefficient (\\(\\exp (0.2076) \\approx 1.23\\))\n\nage:\n\nCoefficient: 0.1487, highly significant.\nSuggests that older firms tend to generate more patents.\n\nage ** 2:\n\nCoefficient: −0.0030, also highly significant.\nIndicates a diminishing return to age: very old firms may become less innovative or productive.\n\nRegion Dummie:\n\nNone of the region dummy variables are statistically significant at the 5% level.\nSuggests no strong regional effect on patent counts after controlling for other factors.\n\n\nTo summarize, the model provides strong evidence that Blueprinty’s software is associated with higher patent productivity. Also, the quadratic form of age shows that patent output rises with experience, but not indefinitely. Regional effects are minimal once other variables are included. These insights can help Blueprinty support its value proposition and target more impactful segments.\n\n\nBecause the coefficient from a Poisson regression model is not directly interpretable as a marginal effect, we estimate the effect of Blueprinty’s software by comparing counterfactual predictions.\nTo do so, we create two hypothetical versions of the dataset:\n\n\\(X_0\\): All firms set to iscustomer = 0 (non-users)\n\\(X_1\\): All firms set to iscustomer = 1 (Blueprinty users)\n\nUsing our fitted model, we compute the expected number of patents under both scenarios:\n\nX_0 = X.copy()\nX_1 = X.copy()\n\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n# Predict expected number of patents under both scenarios\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\n# Compute difference and average effect\ndiff = y_pred_1 - y_pred_0\naverage_effect = diff.mean()\n\nprint(f\"Average treatment effect of Blueprinty's software: {average_effect:.4f} additional patents\")\n\nAverage treatment effect of Blueprinty's software: 0.7928 additional patents\n\n\nThe estimated average treatment effect of Blueprinty’s software is 0.7928. This means that, on average, firms using Blueprinty’s software are predicted to obtain approximately 0.79 more patents over the 5-year period, holding all other firm characteristics constant.\n\n\n\nWe conclude that Blueprinty’s software is associated with a positive and meaningful increase in patent success:\n\nThe marginal effect of using the software is +0.79 patents on average.\nCombined with the earlier result that iscustomer was statistically significant in the regression, this provides strong evidence that Blueprinty’s product has a measurable and valuable effect on innovation outcomes."
  },
  {
    "objectID": "blog/project2/index.html#airbnb-case-study",
    "href": "blog/project2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nIn this case study, we assume the number of reviews is a good proxy for the number of bookings, and we model the number of reviews using listing characteristics.\n\nData Preparation\nWe began by exploring the dataset and found several variables with missing values. To ensure a reliable analysis, we dropped rows missing any of the following variables:\n\nbathrooms\nbedrooms\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\n\nWe also converted categorical variables:\n\nroom_type: one-hot encoded with Entire home/apt as the baseline.\ninstant_bookable: mapped “t” to 1 and “f” to 0.\n\n\n\n\n\n\n\nPython Code for Data Preparation\n\n\n\n\n\n\nimport pandas as pd\nairbnb = pd.read_csv('airbnb.csv')\n\n# Drop rows with missing values in key predictors\nairbnb_clean = airbnb.dropna(subset=[\n    \"bathrooms\",\n    \"bedrooms\",\n    \"review_scores_cleanliness\",\n    \"review_scores_location\",\n    \"review_scores_value\"\n])\n\n# Convert categorical variables\nairbnb_clean[\"instant_bookable\"] = airbnb_clean[\"instant_bookable\"].map({\"t\": 1, \"f\": 0})\n\n# One-hot encode room_type, drop one category to avoid multicollinearity\nroom_dummies = pd.get_dummies(airbnb_clean[\"room_type\"], prefix=\"room\", drop_first=True)\n\n# Construct design matrix\nX = pd.concat([\n    airbnb_clean[[\n        \"price\",\n        \"bedrooms\",\n        \"bathrooms\",\n        \"review_scores_cleanliness\",\n        \"review_scores_location\",\n        \"review_scores_value\",\n        \"instant_bookable\"\n    ]],\n    room_dummies\n], axis=1)\n\n# Add intercept manually\nX.insert(0, \"intercept\", 1)\n\n# Define target variable\nY = airbnb_clean[\"number_of_reviews\"].values\n\n/tmp/ipykernel_97951/2696647610.py:14: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\n\n\nModel Specification\nWe modeled the number of reviews using a Poisson regression, where:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\text{where} \\quad \\lambda_i = \\exp(X_i' \\beta)\n\\]\nThe predictor variables \\(X_i\\) include:\n\nprice, bedrooms, bathrooms\nreview_scores_cleanliness, review_scores_location, review_scores_value\nroom_type dummies\ninstant_bookable\n\n\n\n\n\n\n\nPython Code for Poisson Regression\n\n\n\n\n\n\nimport statsmodels.api as sm\n\n# Ensure all columns in X are numeric\nX = X.astype(float)\n\n# Fit the Poisson regression model\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n# View the results\nprint(poisson_results.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                30160\nModel:                            GLM   Df Residuals:                    30150\nModel Family:                 Poisson   Df Model:                            9\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -5.2900e+05\nDate:                Mon, 05 May 2025   Deviance:                   9.3653e+05\nTime:                        13:12:36   Pearson chi2:                 1.41e+06\nNo. Iterations:                     6   Pseudo R-squ. (CS):             0.5649\nCovariance Type:            nonrobust                                         \n=============================================================================================\n                                coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nintercept                     3.5725      0.016    223.215      0.000       3.541       3.604\nprice                     -1.435e-05    8.3e-06     -1.729      0.084   -3.06e-05    1.92e-06\nbedrooms                      0.0749      0.002     37.698      0.000       0.071       0.079\nbathrooms                    -0.1240      0.004    -33.091      0.000      -0.131      -0.117\nreview_scores_cleanliness     0.1132      0.001     75.820      0.000       0.110       0.116\nreview_scores_location       -0.0768      0.002    -47.796      0.000      -0.080      -0.074\nreview_scores_value          -0.0915      0.002    -50.902      0.000      -0.095      -0.088\ninstant_bookable              0.3344      0.003    115.748      0.000       0.329       0.340\nroom_Private room            -0.0145      0.003     -5.310      0.000      -0.020      -0.009\nroom_Shared room             -0.2519      0.009    -29.229      0.000      -0.269      -0.235\n=============================================================================================\n\n\n\n\n\n\n\nModel Results\nWe estimated a Poisson regression model to understand how listing features are associated with the number of reviews (used here as a proxy for bookings). Coefficients are interpreted on a log scale, so we exponentiate them to obtain multiplicative effects on the expected count.\nCoefficient Interpretation:\n\nintercept:\n\n\\(\\beta_0 = 3.5725\\)\nRepresents the baseline log-expected number of reviews for an entire home/apt, not instantly bookable, with all numeric features equal to zero (not directly meaningful on its own).\n\nprice:\n\nThe coefficient is \\(\\beta = -1.435 \\times 10^{-5},\\ p = 0.084\\).\nNot statistically significant at the 5% level → price does not show a strong effect on review count when controlling for other variables.\n\nbedrooms:\n\nThe coefficient is \\(\\beta = 0.0749,\\ p &lt; 0.001\\).\n\\(\\exp(0.0749)\\approx 1.078\\)\nEach additional bedroom is associated with a 7.8% increase in expected reviews.\n\nbathrooms:\n\nThe coefficient is \\(\\beta = -0.1240,\\ p &lt; 0.001\\).\n\\(\\exp(−0.1240)\\approx 0.883\\)\nEach additional bathroom is associated with an 11.7% decrease in reviews, possibly reflecting shared accommodations.\n\nreview_scores_cleanliness:\n\nThe coefficient is \\(\\beta = 0.1132,\\ p &lt; 0.001\\).\n\\(\\exp(0.1132)\\approx 1.12\\)\nA 1-point increase in cleanliness score is associated with a 12% increase in reviews.\n\nreview_scores_location:\n\nThe coefficient is \\(\\beta = -0.0768,\\ p &lt; 0.001\\).\nCounterintuitive negative effect; may reflect selection bias or underlying location-based review dynamics.\n\nreview_scores_value:\n\nThe coefficient is \\(\\beta = −0.0915,\\ p &lt; 0.001\\).\n\\(\\exp(−0.0915)\\approx 0.91\\)\nHigher value scores are correlated with ~9% fewer reviews, which may not be causal.\n\ninstant_bookable:\n\nThe coefficient is \\(\\beta = 0.3344,\\ p &lt; 0.001\\).\n\\(\\exp(0.3344)\\approx 1.40\\)\nInstantly bookable listings receive ~40% more reviews on average.\n\nroom_Private room (vs. entire home):\n\nThe coefficient is \\(\\beta = −0.0145,\\ p &lt; 0.001\\).\n\\(\\exp(-0.0145) \\approx 0.986\\)\n~1.4% fewer reviews, a small but significant difference.\n\nroom_Shared room (vs. entire home):\n\nThe coefficient is \\(\\beta = −0.2519,\\ p &lt; 0.001\\).\n\\(\\exp(-0.2519) \\approx 0.777\\)\nShared rooms receive ~22% fewer reviews than entire homes.\n\n\n\n\nCONCLUSIONS\nOur findings suggest that:\n\nListings with more bedrooms and those that are instantly bookable receive significantly more reviews.\nShared accommodations and listings with higher “value” ratings receive fewer reviews, perhaps reflecting niche appeal or selection effects.\nThe model explains a considerable amount of variation in review counts (Pseudo R² = 0.5649), indicating strong predictive power."
  },
  {
    "objectID": "HW1/hw1_data.html",
    "href": "HW1/hw1_data.html",
    "title": "Balance Test",
    "section": "",
    "text": "import pandas as pd\ndf = pd.read_stata('karlan_list_2007.dta')\ndf\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.000000\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.000000\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.000000\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50078\n1\n0\n1\n0\n0\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.872797\n0.089959\n0.257265\n2.13\n45047.0\n0.771316\n0.263744\n1.000000\n\n\n50079\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.688262\n0.108889\n0.288792\n2.67\n74655.0\n0.741931\n0.586466\n1.000000\n\n\n50080\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\n0.900000\n0.021311\n0.178689\n2.36\n26667.0\n0.778689\n0.107930\n0.000000\n\n\n50081\n1\n0\n3\n0\n1\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.917206\n0.008257\n0.225619\n2.57\n39530.0\n0.733988\n0.184768\n0.634903\n\n\n50082\n1\n0\n3\n0\n1\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.530023\n0.074112\n0.340698\n3.70\n48744.0\n0.717843\n0.127941\n0.994181\n\n\n\n\n50083 rows × 51 columns\nimport pandas as pd\nimport statsmodels.formula.api as smf\nfrom scipy.stats import t as t_dist\n\n# Variables to test\nvariables = ['mrm2', 'freq', 'hpa', 'female']\n\n# Collect results\nresults = []\n\nfor var in variables:\n    # Split by treatment group\n    treated = df[df[\"treatment\"] == 1][var].dropna()\n    control = df[df[\"treatment\"] == 0][var].dropna()\n\n    # Compute sample stats\n    x_t, x_c = treated.mean(), control.mean()\n    s2_t, s2_c = treated.var(), control.var()\n    n_t, n_c = treated.shape[0], control.shape[0]\n\n    # Manual t-test\n    t_stat = (x_t - x_c) / ((s2_t / n_t + s2_c / n_c) ** 0.5)\n    df_num = (s2_t / n_t + s2_c / n_c) ** 2\n    df_denom = ((s2_t / n_t) ** 2) / (n_t - 1) + ((s2_c / n_c) ** 2) / (n_c - 1)\n    dof = df_num / df_denom\n    p_val_ttest = 2 * t_dist.sf(abs(t_stat), dof)\n\n    # Linear regression\n    formula = f\"{var} ~ treatment\"\n    model = smf.ols(formula, data=df).fit()\n    coef = model.params['treatment']\n    p_val_reg = model.pvalues['treatment']\n\n    # Store results\n    results.append({\n        'Variable': var,\n        'Mean (Treatment)': round(x_t, 3),\n        'Mean (Control)': round(x_c, 3),\n        't-stat (manual)': round(t_stat, 3),\n        'p-value (manual)': round(p_val_ttest, 3),\n        'Coef (regression)': round(coef, 3),\n        'p-value (regression)': round(p_val_reg, 3)\n    })\n\n# Display summary table\nresults_df = pd.DataFrame(results)\nprint(results_df)\n\n  Variable  Mean (Treatment)  Mean (Control)  t-stat (manual)  \\\n0     mrm2            13.012       12.998000            0.120   \n1     freq             8.035        8.047000           -0.111   \n2      hpa            59.597       58.959999            0.970   \n3   female             0.275        0.283000           -1.754   \n\n   p-value (manual)  Coef (regression)  p-value (regression)  \n0             0.905              0.014                 0.905  \n1             0.912             -0.012                 0.912  \n2             0.332              0.637                 0.345  \n3             0.080             -0.008                 0.079"
  },
  {
    "objectID": "HW1/hw1_data.html#experimental-results",
    "href": "HW1/hw1_data.html#experimental-results",
    "title": "Balance Test",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contributions Made\n\n# Create a bar plot of donation rates\ndonation_rates = df.groupby(\"treatment\")[\"gave\"].mean()\ndonation_rates.index = [\"Control\", \"Treatment\"]\nprint(donation_rates)\n\nControl      0.017858\nTreatment    0.022039\nName: gave, dtype: float64\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.bar(donation_rates.index, donation_rates.values)\nplt.ylabel(\"Proportion who donated\")\nplt.title(\"Donation Rates by Group\")\nplt.ylim(0, 0.03)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Manual t-test using class formula\ntreated = df[df[\"treatment\"] == 1][\"gave\"].dropna()\ncontrol = df[df[\"treatment\"] == 0][\"gave\"].dropna()\n\nx_t, x_c = treated.mean(), control.mean()\ns2_t, s2_c = treated.var(), control.var()\nn_t, n_c = len(treated), len(control)\n\nt_stat = (x_t - x_c) / ((s2_t/n_t + s2_c/n_c) ** 0.5)\n\n# Welch–Satterthwaite degrees of freedom\ndf_num = (s2_t/n_t + s2_c/n_c)**2\ndf_denom = ((s2_t/n_t)**2)/(n_t - 1) + ((s2_c/n_c)**2)/(n_c - 1)\ndof = df_num / df_denom\n\n# Two-sided p-value\np_val = 2 * t_dist.sf(abs(t_stat), dof)\n\nprint(f\"Manual t-stat: {t_stat:.3f}\")\nprint(f\"Manual p-value: {p_val:.4f}\")\n\nManual t-stat: 3.209\nManual p-value: 0.0013\n\n\n\n# Linear regression\nmodel_lin = smf.ols(\"gave ~ treatment\", data=df).fit()\nprint(model_lin.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Tue, 22 Apr 2025   Prob (F-statistic):            0.00193\nTime:                        14:37:14   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# Probit regression\nmodel_probit = smf.probit(\"gave ~ treatment\", data=df).fit()\nprint(model_probit.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Tue, 22 Apr 2025   Pseudo R-squ.:               0.0009783\nTime:                        14:36:45   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\n\n\nDifferences between Match Rates\n\n# Subset to treatment group\ntreat_group = df[df[\"treatment\"] == 1]\n\n# Example: compare 2:1 vs 1:1\nx2 = treat_group[treat_group[\"ratio2\"] == 1][\"gave\"]\nx1 = treat_group[treat_group[\"ratio\"] == 1][\"gave\"]\n\nx_t, x_c = x2.mean(), x1.mean()\ns2_t, s2_c = x2.var(), x1.var()\nn_t, n_c = len(x2), len(x1)\n\nt_stat = (x_t - x_c) / ((s2_t/n_t + s2_c/n_c) ** 0.5)\n\n# Welch–Satterthwaite degrees of freedom\ndf_num = (s2_t/n_t + s2_c/n_c)**2\ndf_denom = ((s2_t/n_t)**2)/(n_t - 1) + ((s2_c/n_c)**2)/(n_c - 1)\ndof = df_num / df_denom\n\n# Two-sided p-value\np_val = 2 * t_dist.sf(abs(t_stat), dof)\n\nprint(f\"Manual t-stat: {t_stat:.3f}\")\nprint(f\"Manual p-value: {p_val:.4f}\")\n\nManual t-stat: 0.965\nManual p-value: 0.3345\n\n\n\n# Example: compare 2:1 vs 3:1\nx2 = treat_group[treat_group[\"ratio2\"] == 1][\"gave\"]\nx3 = treat_group[treat_group[\"ratio\"] == 1][\"gave\"]\n\nx_t, x_c = x2.mean(), x3.mean()\ns2_t, s2_c = x2.var(), x3.var()\nn_t, n_c = len(x2), len(x3)\n\nt_stat = (x_t - x_c) / ((s2_t/n_t + s2_c/n_c) ** 0.5)\n\n# Welch–Satterthwaite degrees of freedom\ndf_num = (s2_t/n_t + s2_c/n_c)**2\ndf_denom = ((s2_t/n_t)**2)/(n_t - 1) + ((s2_c/n_c)**2)/(n_c - 1)\ndof = df_num / df_denom\n\n# Two-sided p-value\np_val = 2 * t_dist.sf(abs(t_stat), dof)\n\nprint(f\"Manual t-stat: {t_stat:.3f}\")\nprint(f\"Manual p-value: {p_val:.4f}\")\n\nManual t-stat: 0.965\nManual p-value: 0.3345\n\n\n\n# Example: compare 1:1 vs 3:1\nx1 = treat_group[treat_group[\"ratio2\"] == 1][\"gave\"]\nx3 = treat_group[treat_group[\"ratio\"] == 1][\"gave\"]\n\nx_t, x_c = x1.mean(), x3.mean()\ns2_t, s2_c = x1.var(), x3.var()\nn_t, n_c = len(x1), len(x3)\n\nt_stat = (x_t - x_c) / ((s2_t/n_t + s2_c/n_c) ** 0.5)\n\n# Welch–Satterthwaite degrees of freedom\ndf_num = (s2_t/n_t + s2_c/n_c)**2\ndf_denom = ((s2_t/n_t)**2)/(n_t - 1) + ((s2_c/n_c)**2)/(n_c - 1)\ndof = df_num / df_denom\n\n# Two-sided p-value\np_val = 2 * t_dist.sf(abs(t_stat), dof)\n\nprint(f\"Manual t-stat: {t_stat:.3f}\")\nprint(f\"Manual p-value: {p_val:.4f}\")\n\nManual t-stat: 0.965\nManual p-value: 0.3345\n\n\n\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\n# Run regression with 1:1 as baseline (ratio1 is omitted)\nmodel = smf.ols(\"gave ~ ratio2 + ratio3\", data=treat_group).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Tue, 22 Apr 2025   Prob (F-statistic):              0.524\nTime:                        14:36:45   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0207      0.001     14.912      0.000       0.018       0.023\nratio2         0.0019      0.002      0.958      0.338      -0.002       0.006\nratio3         0.0020      0.002      1.008      0.313      -0.002       0.006\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         3.73\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\n# Raw response rates by match ratio\n\n# Create match ratio group labels\ndef get_ratio(row):\n    if row[\"ratio2\"] == 1:\n        return \"2:1\"\n    elif row[\"ratio3\"] == 1:\n        return \"3:1\"\n    else:\n        return \"1:1\"\n\ntreat_group[\"ratio_group\"] = treat_group.apply(get_ratio, axis=1)\n\n# Calculate mean donation rate by group\ndonation_rates = treat_group.groupby(\"ratio_group\")[\"gave\"].mean()\nprint(\"Raw Donation Rates by Match Ratio:\\n\", donation_rates)\n\n# --- 2. Differences in fitted coefficients (from previous regression) ---\nmodel = smf.ols(\"gave ~ ratio2 + ratio3\", data=treat_group).fit()\ncoef_1_1 = model.params['Intercept']\ncoef_2_1 = coef_1_1 + model.params['ratio2']\ncoef_3_1 = coef_1_1 + model.params['ratio3']\n\nprint(\"\\nFitted Coefficients:\")\nprint(f\"1:1 donation rate (Intercept): {coef_1_1:.4f}\")\nprint(f\"2:1 estimated rate: {coef_2_1:.4f}\")\nprint(f\"3:1 estimated rate: {coef_3_1:.4f}\")\n\n# --- 3. Compute response rate differences ---\nprint(\"\\nDifferences in response rates (from regression):\")\nprint(f\"2:1 − 1:1: {model.params['ratio2']:.4f}\")\nprint(f\"3:1 − 1:1: {model.params['ratio3']:.4f}\")\nprint(f\"3:1 − 2:1: {model.params['ratio3'] - model.params['ratio2']:.4f}\")\n\nRaw Donation Rates by Match Ratio:\n ratio_group\n1:1    0.020749\n2:1    0.022633\n3:1    0.022733\nName: gave, dtype: float64\n\nFitted Coefficients:\n1:1 donation rate (Intercept): 0.0207\n2:1 estimated rate: 0.0226\n3:1 estimated rate: 0.0227\n\nDifferences in response rates (from regression):\n2:1 − 1:1: 0.0019\n3:1 − 1:1: 0.0020\n3:1 − 2:1: 0.0001\n\n\n/tmp/ipykernel_5387/3286036591.py:15: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  treat_group[\"ratio_group\"] = treat_group.apply(get_ratio, axis=1)\n\n\n\n# Raw response rates by match ratio\n\n# Create match ratio group labels\ndef get_ratio(row):\n    if row[\"ratio2\"] == 1:\n        return \"2:1\"\n    elif row[\"ratio3\"] == 1:\n        return \"3:1\"\n    else:\n        return \"1:1\"\n\ntreat_group[\"ratio_group\"] = treat_group.apply(get_ratio, axis=1)\n\n/tmp/ipykernel_5387/716834291.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  treat_group[\"ratio_group\"] = treat_group.apply(get_ratio, axis=1)\n\n\n\n# Create match ratio group labels\ndef get_ratio(row):\n    if row[\"ratio2\"] == 1:\n        return \"2:1\"\n    elif row[\"ratio3\"] == 1:\n        return \"3:1\"\n    else:\n        return \"1:1\"\n\n# Safely add the new column\ntreat_group.loc[:, \"ratio_group\"] = treat_group.apply(get_ratio, axis=1)\n\n# Calculate mean donation rate by group\ndonation_rates = treat_group.groupby(\"ratio_group\")[\"gave\"].mean()\nprint(\"Raw Donation Rates by Match Ratio:\\n\", donation_rates)\n\n\nRaw Donation Rates by Match Ratio:\n ratio_group\n1:1    0.020749\n2:1    0.022633\n3:1    0.022733\nName: gave, dtype: float64\n\n\n\n\nSize of Charitable Contribution\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\n# Compare donation amounts across all individuals\n# (most people donated $0)\ntreated = df[df[\"treatment\"] == 1][\"amount\"].dropna()\ncontrol = df[df[\"treatment\"] == 0][\"amount\"].dropna()\n\n# T-test\nt_stat, p_val = ttest_ind(treated, control, equal_var=False)\nprint(f\"T-test: t = {t_stat:.3f}, p = {p_val:.4f}\")\n\n# Linear regression (everyone)\nmodel_all = smf.ols(\"amount ~ treatment\", data=df).fit()\nprint(model_all.summary())\n\nT-test: t = 1.918, p = 0.0551\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.461\nDate:                Tue, 22 Apr 2025   Prob (F-statistic):             0.0628\nTime:                        14:36:46   Log-Likelihood:            -1.7946e+05\nNo. Observations:               50083   AIC:                         3.589e+05\nDf Residuals:                   50081   BIC:                         3.589e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.8133      0.067     12.063      0.000       0.681       0.945\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\nOmnibus:                    96861.113   Durbin-Watson:                   2.008\nProb(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\nSkew:                          15.297   Prob(JB):                         0.00\nKurtosis:                     341.269   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# Filter only donors (those who gave a positive amount)\ndonors = df[df[\"gave\"] == 1]\n\n# Compare mean donation amounts between treatment & control among donors\ntreated_donors = donors[donors[\"treatment\"] == 1][\"amount\"]\ncontrol_donors = donors[donors[\"treatment\"] == 0][\"amount\"]\n\nt_stat, p_val = ttest_ind(treated_donors, control_donors, equal_var=False)\nprint(f\"T-test (donors only): t = {t_stat:.3f}, p = {p_val:.4f}\")\n\n# Run regression: amount ~ treatment\nmodel_donors = smf.ols(\"amount ~ treatment\", data=donors).fit()\n\n# Output summary\nprint(model_donors.summary())\n\nT-test (donors only): t = -0.585, p = 0.5590\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.3374\nDate:                Tue, 22 Apr 2025   Prob (F-statistic):              0.561\nTime:                        14:36:46   Log-Likelihood:                -5326.8\nNo. Observations:                1034   AIC:                         1.066e+04\nDf Residuals:                    1032   BIC:                         1.067e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\nOmnibus:                      587.258   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5623.279\nSkew:                           2.464   Prob(JB):                         0.00\nKurtosis:                      13.307   Cond. No.                         3.49\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nimport matplotlib.pyplot as plt\n\n# Subset only those who donated\ndonors = df[df[\"gave\"] == 1]\n\n# Split by treatment\ntreatment_donors = donors[donors[\"treatment\"] == 1][\"amount\"]\ncontrol_donors = donors[donors[\"treatment\"] == 0][\"amount\"]\n\n# Compute means\nmean_treat = treatment_donors.mean()\nmean_control = control_donors.mean()\n\n# Plot histograms\nfig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n\n# Control group plot\naxes[0].hist(control_donors, bins=30, color='skyblue', edgecolor='black')\naxes[0].axvline(mean_control, color='red', linestyle='dashed', linewidth=2, label=f\"Mean = {mean_control:.2f}\")\naxes[0].set_title(\"Control Group\")\naxes[0].set_xlabel(\"Donation Amount\")\naxes[0].set_ylabel(\"Frequency\")\naxes[0].legend()\n\n# Treatment group plot\naxes[1].hist(treatment_donors, bins=30, color='lightgreen', edgecolor='black')\naxes[1].axvline(mean_treat, color='red', linestyle='dashed', linewidth=2, label=f\"Mean = {mean_treat:.2f}\")\naxes[1].set_title(\"Treatment Group\")\naxes[1].set_xlabel(\"Donation Amount\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "HW1/hw1_data.html#simulation-experiment",
    "href": "HW1/hw1_data.html#simulation-experiment",
    "title": "Balance Test",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\n\nLaw of Large Numbers\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Simulation parameters\nn = 10000\np_control = 0.018\np_treatment = 0.022\n\n# Simulate donations (1 = gave, 0 = didn't)\ncontrol_draws = np.random.binomial(1, p_control, n)\ntreatment_draws = np.random.binomial(1, p_treatment, n)\n\n# Compute stepwise differences in donation outcomes\ndiffs = treatment_draws - control_draws\n\n# Cumulative average of differences\ncumulative_avg_diff = np.cumsum(diffs) / np.arange(1, n+1)\n\n# Plot\nplt.figure(figsize=(10, 4))\nplt.plot(cumulative_avg_diff, label='Cumulative Average Difference', color='blue')\nplt.axhline(p_treatment - p_control, color='red', linestyle='dashed', label=f\"True Diff = {p_treatment - p_control:.004f}\")\nplt.title(\"Law of Large Numbers: Cumulative Average Difference\")\nplt.xlabel(\"Simulation Iteration\")\nplt.ylabel(\"Average Difference in Donation Rates\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed\nnp.random.seed(42)\n\n# Parameters\np_control = 0.018\np_treatment = 0.022\nsample_sizes = [50, 200, 500, 1000]\nnum_simulations = 1000\n\n# Create subplots\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    avg_diffs = []\n\n    for _ in range(num_simulations):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        avg_diffs.append(diff)\n\n    # Plot histogram\n    axes[i].hist(avg_diffs, bins=30, color='lightblue', edgecolor='black')\n    axes[i].axvline(x=0, color='red', linestyle='dashed', label='Zero')\n    axes[i].axvline(x=np.mean(avg_diffs), color='black', linestyle='dotted', label='Sample Mean')\n    axes[i].set_title(f\"Sample Size = {n}\")\n    axes[i].set_xlabel(\"Avg. Difference in Donation Rates\")\n    axes[i].set_ylabel(\"Frequency\")\n    axes[i].legend()\n\nplt.suptitle(\"Central Limit Theorem: Distribution of Average Differences\", fontsize=14)\nplt.tight_layout()\nplt.subplots_adjust(top=0.9)\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sara Antentas Oliveras",
    "section": "",
    "text": "MSBA Student @ UC San Diego - Rady School of Management\n\nExperience:\n\nTeaching Assistant | UC San Diego\n\n\n\nEducation:\n\nBachelor’s degree in Mathematics\nBachelor’s degree in Physics"
  },
  {
    "objectID": "HW3/hw3_questions.html",
    "href": "HW3/hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment explores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "HW3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "HW3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "HW3/hw3_questions.html#simulate-conjoint-data",
    "href": "HW3/hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed."
  },
  {
    "objectID": "HW3/hw3_questions.html#preparing-the-data-for-estimation",
    "href": "HW3/hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps.\nBefore estimation, we need to:\n\nConvert categorical variables into numeric format (i.e., dummy variables).\nEnsure that each respondent-task combination has exactly 3 alternatives.\nReshape the data into “long format,” suitable for conditional logit models.\n\nHere’s how we prepped the data in Python:\n\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv(\"conjoint_data.csv\")\n\n# One-hot encode categorical variables: brand and ad\ndf_encoded = pd.get_dummies(df, columns=['brand', 'ad'], drop_first=True)\n\n# Sort by respondent and task\ndf_encoded = df_encoded.sort_values(by=['resp', 'task']).reset_index(drop=True)\n\n# Ensure each task has 3 alternatives\ntask_counts = df_encoded.groupby(['resp', 'task']).size()\nassert (task_counts == 3).all(), \"Each task should have 3 alternatives\"\n\nThis gives us a clean, numeric dataset where each row represents a single product alternative within a choice task. The outcome variable choice is binary (1 = chosen), and the features include price and dummy variables for brand and advertising status."
  },
  {
    "objectID": "HW3/hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "HW3/hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nTo estimate the parameters of the Multinomial Logit (MNL) model, we construct and maximize the log-likelihood function using Python’s scipy.optimize.minimize() function with the BFGS algorithm.\nWe model utility as a linear function of product attributes, and use the softmax function to translate utilities into choice probabilities. Specifically, for each choice task with 3 alternatives, the probability that alternative \\(j\\) is chosen is:\n\\[\nP_{ij} = \\frac{\\exp(U_{ij})}{\\sum_{k=1}^{3} \\exp(U_{ik})}\n\\]\nThe log-likelihood function sums the log of the chosen alternative’s probability across all tasks. We minimize its negative to perform maximum likelihood estimation.\nHere is the Python implementation:\n\nfrom scipy.optimize import minimize\nimport numpy as np\nimport pandas as pd\n\n# Convert all features to float\nX = df_encoded[['brand_N', 'brand_P', 'ad_Yes', 'price']].astype(float).values\ny = df_encoded['choice'].values\n\n# Reshape into groups of 3 alternatives per task\nn_alternatives = 3\nn_obs = len(y) // n_alternatives\nX_grouped = X.reshape((n_obs, n_alternatives, -1))\ny_grouped = y.reshape((n_obs, n_alternatives))\n\n# Define the negative log-likelihood\ndef neg_log_likelihood(beta):\n    ll = 0\n    for i in range(n_obs):\n        utilities = X_grouped[i] @ beta\n        probs = np.exp(utilities) / np.sum(np.exp(utilities))\n        ll += np.log(probs[y_grouped[i] == 1][0])\n    return -ll\n\n# Estimate parameters\ninitial_beta = np.zeros(X.shape[1])\nresult = minimize(neg_log_likelihood, initial_beta, method='BFGS')\n\n# Extract coefficients, standard errors, and confidence intervals\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\nci_lower = beta_hat - 1.96 * standard_errors\nci_upper = beta_hat + 1.96 * standard_errors\n\n# Summarize results\nsummary_df = pd.DataFrame({\n    'Coefficient': beta_hat,\n    'Std. Error': standard_errors,\n    '95% CI Lower': ci_lower,\n    '95% CI Upper': ci_upper\n}, index=['brand_N', 'brand_P', 'ad_Yes', 'price'])\n\nsummary_df\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n95% CI Lower\n95% CI Upper\n\n\n\n\nbrand_N\n0.941195\n0.018302\n0.905323\n0.977068\n\n\nbrand_P\n0.501616\n0.004777\n0.492252\n0.510979\n\n\nad_Yes\n-0.731994\n0.004563\n-0.740938\n-0.723051\n\n\nprice\n-0.099480\n0.005067\n-0.109413\n-0.089548\n\n\n\n\n\n\n\n\nInterpretation:\n\nNetflix is strongly preferred over the baseline brand (Hulu).\nPrime is also preferred, but to a lesser extent than Netflix.\nAds significantly reduce utility—consumers dislike ads.\nAs expected, higher prices lower the probability of choice.\n\n\n\nConclusion:\nAll coefficients are statistically significant (very tight confidence intervals), and the signs are intuitive:\n\nConsumers prefer Netflix and Prime over Hulu.\nAds and price negatively impact product choice.\n\nThese insights could guide product positioning, pricing, and ad strategy."
  },
  {
    "objectID": "HW3/hw3_questions.html#estimation-via-bayesian-methods",
    "href": "HW3/hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\nTo estimate the posterior distribution of the MNL model parameters, we implemented a Metropolis-Hastings MCMC sampler. We ran the algorithm for 11,000 iterations, discarding the first 1,000 as burn-in, and retained the remaining 10,000 draws for inference.\nWe specified normal priors as follows:\n\nFor binary predictors (brand_N, brand_P, ad_Yes): \\[\n\\beta_k \\sim \\mathcal{N}(0, 5)\n\\]\nFor the continuous predictor (price): \\[\n\\beta_{\\text{price}} \\sim \\mathcal{N}(0, 1)\n\\]\n\nThese priors reflect weak initial beliefs centered at zero, with more regularization for price due to its different scale.\nAdditionally, instead of computing the posterior as likelihood x prior, we worked in log-space for numerical stability: \\[\n\\log(\\text{posterior}) = \\log(\\text{likelihood}) + \\log(\\text{prior})\n\\]\nThis enabled us to reuse the log-likelihood function from the MLE section and simply add the log-prior density.\nWe used a diagonal multivariate normal proposal distribution:\n\\[\n\\text{Proposal: } \\beta^* = \\beta^{(t)} + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\Sigma)\n\\]\nWhere:\n\\[\n\\Sigma = \\text{diag}(0.05^2, 0.05^2, 0.05^2, 0.005^2)\n\\]\nThis means each component was perturbed independently:\n\nThe first 3 (\\(\\beta_{\\text{brand\\_N}}, \\beta_{\\text{brand\\_P}}, \\beta_{\\text{ad\\_Yes}}\\)) used \\(\\mathcal{N}(0, 0.05^2)\\)\n\nThe price coefficient used \\(\\mathcal{N}(0, 0.005^2)\\)\n\n\nPosterior Results:\n\nimport pandas as pd\nimport numpy as np\n\n# Log-prior: N(0, 0.05^2) for binary vars, N(0, 0.005^2) for price\ndef log_prior(beta):\n    prior_sd = np.array([0.05, 0.05, 0.05, 0.005])\n    return -0.5 * np.sum((beta / prior_sd)**2) - np.sum(np.log(prior_sd * np.sqrt(2 * np.pi)))\n\n# Log-posterior\ndef log_posterior(beta):\n    return -neg_log_likelihood(beta) + log_prior(beta)\n\n# MCMC sampler\ndef metropolis_sampler(log_post, initial, n_samples=3000, proposal_sd=[0.05, 0.05, 0.05, 0.005]):\n    samples = [initial]\n    current_log_post = log_post(initial)\n    np.random.seed(0)\n    for _ in range(n_samples - 1):\n        proposal = samples[-1] + np.random.normal(scale=proposal_sd)\n        proposal_log_post = log_post(proposal)\n        log_accept_ratio = proposal_log_post - current_log_post\n        if np.log(np.random.rand()) &lt; log_accept_ratio:\n            samples.append(proposal)\n            current_log_post = proposal_log_post\n        else:\n            samples.append(samples[-1])\n    return np.array(samples)\n\n# Run it\ninitial_beta = np.zeros(X.shape[1])\nmcmc_samples = metropolis_sampler(log_posterior, initial_beta)\n\n# Discard burn-in\nmcmc_samples = mcmc_samples[1000:]\n\n# Posterior summary\nposterior_means = mcmc_samples.mean(axis=0)\nposterior_sds = mcmc_samples.std(axis=0)\nposterior_ci_lower = np.percentile(mcmc_samples, 2.5, axis=0)\nposterior_ci_upper = np.percentile(mcmc_samples, 97.5, axis=0)\n\nposterior_summary = pd.DataFrame({\n    \"Posterior Mean\": posterior_means,\n    \"Posterior SD\": posterior_sds,\n    \"95% CI Lower\": posterior_ci_lower,\n    \"95% CI Upper\": posterior_ci_upper\n}, index=['brand_N', 'brand_P', 'ad_Yes', 'price'])\n\nprint(posterior_summary)\n\n         Posterior Mean  Posterior SD  95% CI Lower  95% CI Upper\nbrand_N        0.150717      0.047870      0.064154      0.248803\nbrand_P        0.024741      0.040724     -0.059125      0.104080\nad_Yes        -0.182542      0.043102     -0.265188     -0.092351\nprice         -0.041593      0.003704     -0.049004     -0.034733\n\n\n\n\nConvergence Diagnostics:\n\nimport matplotlib.pyplot as plt\n\n# Trace plot for price\nplt.figure(figsize=(12, 4))\nplt.plot(mcmc_samples[:, 3])\nplt.title(\"Trace Plot for β_price\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"β_price\")\nplt.grid(True)\nplt.show()\n\n# Histogram for price\nplt.hist(mcmc_samples[:, 3], bins=40, edgecolor='black')\nplt.title(\"Posterior Distribution for β_price\")\nplt.xlabel(\"β_price\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor \\(\\beta_{\\text{price}}\\), we show the trace plot and posterior histogram:\n\nThe trace plot indicates good mixing — no visible trends, and the chain explores the parameter space well.\nThe posterior histogram shows a clean, unimodal distribution, consistent with convergence.\n\n\n\nComparison with Maximum Likelihood Estimates:\nThe Bayesian posterior summaries reveal some key differences compared to the Maximum Likelihood Estimation (MLE) results. Across all four parameters, the posterior means are shrunk toward zero, a typical effect of incorporating prior information. This shrinkage is most pronounced for brand_P, where the MLE estimate is 0.5016, while the Bayesian posterior mean is only 0.0247 with a wide 95% credible interval spanning zero. This suggests greater uncertainty and weaker identification for that parameter, which the prior tempers accordingly.\nFor brand_N and ad_Yes, the Bayesian estimates maintain the same directional effects as the MLE (positive and negative, respectively), but again with reduced magnitude. Notably, the price coefficient remains clearly negative in both approaches, confirming its importance, but the Bayesian estimate is more conservative (−0.0416 vs. −0.0995) and comes with a narrower 95% credible interval. This reflects the influence of the tighter prior on the price parameter and the Bayesian framework’s regularization of uncertainty.\nOverall, the Bayesian results reinforce the main conclusions from the MLE model, while offering a more nuanced interpretation grounded in prior beliefs and sampling uncertainty."
  },
  {
    "objectID": "HW3/hw3_questions.html#discussion",
    "href": "HW3/hw3_questions.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\ntodo: Suppose you did not simulate the data. What do you observe about the parameter estimates? What does \\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\) mean? Does it make sense that \\(\\beta_\\text{price}\\) is negative?\ntodo: At a high level, discuss what change you would need to make in order to simulate data from — and estimate the parameters of — a multi-level (aka random-parameter or hierarchical) model. This is the model we use to analyze “real world” conjoint data.\nIf we treat this as real (non-simulated) data, the parameter estimates provide interpretable insights into consumer preferences. The fact that \\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\) indicates that, on average, consumers have a stronger preference for Netflix compared to Prime, when holding other factors constant. This kind of insight is crucial for positioning and competitive analysis. Similarly, the negative estimate for \\(\\beta_\\text{price}\\) aligns with economic theory — as price increases, choice probability decreases. This confirms that price sensitivity is present and measurable in the model.\nThe interpretation of the coefficients is straightforward in a Multinomial Logit model: each \\(\\beta_\\text{k}\\) reflects the relative utility contribution of that attribute level. In practice, these parameters can be used to simulate market shares, conduct “what-if” pricing analysis, or design new product bundles.\n\nExtending to a Multi-Level Model\nTo better capture heterogeneity in preferences, we would shift from this standard (pooled) MNL to a multi-level or random-coefficient model (also called Hierarchical Bayes (HB) MNL). This extension allows each respondent to have their own set of \\(\\beta_\\text{i}\\), drawn from a population distribution:\n\\[\n\\beta_i \\sim \\mathcal{N}(\\mu, \\Sigma)\n\\]\nWhere:\n\n\\(\\mu\\) represents the population mean of preferences\n\n\\(\\Sigma\\) captures the variance and covariances between individual-level preferences\n\nTo simulate from such a model:\n\nWe would first draw \\(\\beta_i\\) for each respondent from \\(\\mathcal{N}(\\mu, \\Sigma\\).\nThen simulate their choices based on their individual utilities.\n\nTo estimate it:\n\nWe would need multiple choice tasks per respondent (which we already have),\n\nAnd use either:\n\nHierarchical Bayesian methods (e.g., MCMC with Gibbs sampling), or\n\nMixed Logit via simulated maximum likelihood.\n\n\nThis structure better reflects real-world behavior, where different people have different sensitivities to brand, price, and advertising, and allows for more personalized predictions and richer segmentation."
  }
]